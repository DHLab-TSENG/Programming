---
title: "6. Pandas"
author: "Yi-Ju Tseng"
format:
  revealjs:
    slide-number: c/t
    show-slide-number: all
editor: visual
---

## Pandas

-   Introduction to Pandas Objects
-   Data Indexing and Selection in Pandas
-   Operations in Pandas
-   Handling Missing Data in Pandas
-   Hierarchical Indexing in Pandas
-   Combining Datasets: Concat and Append
-   Combining Datasets: Merge and Join
-   Aggregation and Grouping in Pandas
-   Working with Time Series in Pandas

# Introduction to Pandas Objects

## Introduction to Pandas Objects

## Overview

-   Pandas objects are enhanced versions of NumPy structured arrays
    -   we need more flexibility (e.g., attaching labels to data, working with missing data, etc.)
-   Rows and columns identified with labels instead of integer indices
-   Three fundamental data structures:
    -   Series
    -   DataFrame
    -   Index

## Pandas Getting Started

Install

```{python}
#| eval: false
#| echo: true
!pip3 install numpy pandas # ! for shell script
```

Import

```{python}
#| echo: true
import numpy as np
import pandas as pd
```

## The Pandas Series Object

-   One-dimensional array of indexed data
-   Wraps both values and indices

```{python}
#| echo: true
data = pd.Series([0.25, 0.5, 0.75, 1.0])
data
```

## Series: Data Access

-   Position (index)
-   Slicing

```{python}
#| echo: true
# Access by position
data[1] 
# Slicing works too
data[1:3]    
```

## Series: Flexible Indices

-   Pandas Series has an explicitly defined index associated with the values
-   Index need not be integers
-   Can use strings or other types

```{python}
#| echo: true
data = pd.Series([0.25, 0.5, 0.75, 1.0],
                 index=['a', 'b', 'c', 'd'])
data['b']
```

## Series as Specialized Dictionary

-   keys: index
-   values: values

```{python}
#| echo: true
population_dict = {'California': 38332521,
                   'Texas': 26448193,
                   'New York': 19651127,
                   'Florida': 19552860,
                   'Illinois': 12882135}
population = pd.Series(population_dict)
print(population)
```

------------------------------------------------------------------------

## Series: Dictionary-like Operations

-   Index (Dictionary-style access)
-   Slicing! (Array-style operations)

```{python}
#| echo: true
population['California'] 
```

```{python}
#| echo: true
population['California':'Illinois']
```

## Series Construction

-   From list (default integer index)
-   Scalar value with custom index
-   From dictionary

```{python}
#| echo: true
pd.Series([2, 4, 6])
```

```{python}
#| echo: true
pd.Series(5, index=[100, 200, 300])
```

```{python}
#| echo: true
pd.Series({2:'a', 1:'b', 3:'c'})
```

## Hands-on - Series

-   Create a Pandas Series containing the following student scores (out of 100): 85, 92, 78, 90, 88.
-   Create a Series with the same scores but use student names as the index: 'Alice', 'Bob', 'Charlie', 'David', 'Emma'.
-   Series Operations
-   Find Bob's score
-   Find the average score
-   Find all scores above 85
-   Add 5 points to everyone's score

## The Pandas DataFrame Object

-   Two-dimensional data structure
    -   with both flexible row indices and flexible column names
-   Can be viewed as:
    -   Generalized NumPy array
    -   Specialization of Python dictionary
    -   Collection of aligned Series objects

## DataFrame: Example

```{python}
#| echo: true
# Create area Series
area_dict = {'California': 423967, 'Texas': 695662, 
             'New York': 141297, 'Florida': 170312, 
             'Illinois': 149995}
area = pd.Series(area_dict)

# Combine with population Series
states = pd.DataFrame({'population': population,
                       'area': area})
states
```

------------------------------------------------------------------------

## DataFrame: Structure

-   Access index (row labels)
-   Access columns

```{python}
#| echo: true
print(states.index)
print(states.columns)
```

## DataFrame as Dictionary of Series

Access a column (returns a Series) with column name and `[]`

```{python}
#| echo: true
states['area']
```

## DataFrame Construction Methods

-   From a single Series
-   From a list of dictionaries
-   From a dictionary of Series
-   From a two-dimensional NumPy array

## DataFrame: From a Single Series

```{python}
pd.DataFrame(population, columns=['population'])
```

## DataFrame: From List of Dicts

```{python}
#| echo: true
data = [{'a': i, 'b': 2 * i} for i in range(3)]
print(data)
```

```{python}
pd.DataFrame(data)
```

```{python}
#| echo: true
pd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])
```

## DataFrame: From a Dictionary of Series

```{python}
pd.DataFrame({'population': population,
              'area': area})
```

## DataFrame: From 2D NumPy Array

```{python}
#| echo: true
pd.DataFrame(np.random.rand(3, 2),
             columns=['foo', 'bar'],
             index=['a', 'b', 'c'])
```

## The Pandas Index Object

-   Used by both Series and DataFrame
-   Can be viewed as:
    -   Immutable array
    -   Ordered set

```{python}
#| echo: true
ind = pd.Index([2, 3, 5, 7, 11])
```

## Summary

-   **Series**: 1D array with labeled index
-   **DataFrame**: 2D table with labeled rows and columns
-   **Index**: Immutable array-like structure for labeling

## Hands-on - DataFrame

You have information about countries stored in dictionaries. Convert this to a DataFrame `countries_df`.

```         
# Population
population_dict = {
    'USA': 331002651,
    'China': 1439323776,
    'India': 1380004385,
    'Brazil': 212559417,
    'Japan': 126476461
}
# Area
area_dict = {
    'USA': 9833517,
    'China': 9596960,
    'India': 3287263,
    'Brazil': 8515767,
    'Japan': 377975
}
#GDP
gdp_dict = {
    'USA': 21433225,
    'China': 14342903,
    'India': 2875142,
    'Brazil': 1839758,
    'Japan': 5081770
}
```

## Hands-on - DataFrame

-   Add the following columns to the countries_df:
    -   Population density (population per square km)
    -   GDP per capita

# Data Indexing and Selection in Pandas

## Series Indexing - 1/2

Pandas Series objects support multiple indexing methods

-   **Dictionary-style indexing**: Access values by index (labels)

```{python}
#| echo: true
import pandas as pd
data = pd.Series([0.25, 0.5, 0.75, 1.0],
                 index=['a', 'b', 'c', 'd'])
data
```

```{python}
data['b']  
```

-   **Attribute-style access**: For string index (labels)

```{python}
  #| echo: true
  data.area  # Same as data['area']
```

## Series Indexing - 2/2

-   **Slicing with explicit index**: Includes the end point

```{python}
  #| echo: true
  data['a':'c']  # Includes 'a', 'b', 'c'
```

-   **Slicing with implicit integer index**: Excludes the end point

```{python}
  #| echo: true
  data[0:2]  # Includes indices 0, 1
```

-   **Boolean masking**: Filter based on conditions

```{python}
  #| echo: true
  data[(data > 0.3) & (data < 0.8)]   
```

## Special Indexers

Pandas provides special indexers to avoid confusion with integer indices:

-   **loc**: Label-based indexing

```{python}
  #| echo: true
data.loc[1]  
```

-   **iloc**: Integer-based indexing

```{python}
  #| echo: true
data.iloc[1]  
```

## DataFrame Indexing

```{python}
#| echo: true
area = pd.Series({'California': 423967, 'Texas': 695662,
                  'New York': 141297, 'Florida': 170312,
                  'Illinois': 149995})
pop = pd.Series({'California': 38332521, 'Texas': 26448193,
                 'New York': 19651127, 'Florida': 19552860,
                 'Illinois': 12882135})
df = pd.DataFrame({'area':area, 'pop':pop})
df
```

## DataFrame Indexing

DataFrames can be indexed as dictionaries of Series or as two-dimensional arrays:

-   **Column access**:

```{python}
#| echo: true
df['area']  # Dictionary-style
df.area     # Attribute-style
```

-   **Adding new columns**:

```{python}
  #| echo: true
df['density'] = df['pop'] / df['area']
```

## Special Indexers

-   **loc**: Label-based indexing

```{python}
  #| echo: true
data.loc[:'Illinois', :'pop']
```

-   **iloc**: Integer-based indexing

```{python}
  #| echo: true
data.iloc[:3, :2]
```

## Additional Indexing Conventions

Slicing refers to rows when used directly on a DataFrame:

```{python}
  #| echo: true
# Selects rows with these index (labels)
data['Florida':'Illinois']  
```

```{python}
#| echo: true
# Selects rows by position
data[1:3]                  
```

## Additional Indexing Conventions

Direct masking is row-wise:

```{python}
#| echo: true
# Selects rows where density > 100
data[data.density > 100]   
```

## Hands-on - Index and Slicing

```{python}
#| echo: true
import pandas as pd
import numpy as np
# Create Series for COVID-19 metrics
cases = pd.Series({'United States': 98.2, 'India': 44.7, 'Brazil': 37.6,'France': 35.4, 'Germany': 30.2})
deaths = pd.Series({'United States': 1.1, 'India': 0.53, 'Brazil': 0.69,'France': 0.16, 'Germany': 0.17})
population = pd.Series({'United States': 331.9, 'India': 1393.4, 'Brazil': 214.3, 'France': 67.8, 'Germany': 83.2})
# Create DataFrame
covid_df = pd.DataFrame({'cases_millions': cases, 
                         'deaths_millions': deaths, 
                         'population_millions': population})
print("COVID-19 Statistics by Country:")
covid_df
```

## Hands-on - Index and Slicing

-   Access the number of cases in Brazil
-   Add a new column 'case_fatality_rate' calculated as deaths divided by cases (as a percentage)
-   Use loc to select data for Brazil and France, showing only cases and deaths
-   Use iloc to select the first 3 countries and all columns
-   Select countries with case fatality rate greater than 1.5%

# Operations in Pandas

## Universal Functions: Index Preservation

Pandas inherits NumPy's ability to perform quick element-wise operations while adding two key features:

-   **Index and column label preservation** for unary operations
-   **Automatic index alignment** for binary operations

## Universal Functions: Index Preservation

The indices are preserved

```{python}
#| echo: true
import pandas as pd
import numpy as np
# Create sample data
rng = np.random.RandomState(42)
ser = pd.Series(rng.randint(0, 10, 4))
print(ser)
```

```{python}
#| echo: true
np.exp(ser)
```

## Universal Functions: Index Preservation

The indices are preserved

```{python}
#| echo: true
df = pd.DataFrame(rng.randint(0, 10, (3, 4)),
                 columns=['A', 'B', 'C', 'D'])
print(df)                
```

```{python}
#| echo: true
np.sin(df * np.pi / 4)
```

## Index Alignment in Operations

When operating on two Series, Pandas automatically aligns indices:

```{python}
#| echo: true
area = pd.Series({'Alaska': 1723337, 'Texas': 695662,'California': 423967}, name='area')
population = pd.Series({'California': 38332521, 'Texas': 26448193, 'New York': 19651127}, name='population')
# Division automatically aligns indices
population / area
```

## Index Alignment in Operations

```{python}
#| echo: true
A = pd.Series([2, 4, 6], index=[0, 1, 2])
B = pd.Series([1, 3, 5], index=[1, 2, 3])
```

Default behavior produces NaN for missing indices

```{python}
A + B
```

Using add() method with fill_value

```{python}
A.add(B, fill_value=0)
```

## DataFrame Alignment

For DataFrames, alignment happens on both rows and columns.

```{python}
#| echo: true
A = pd.DataFrame(rng.randint(0, 20, (2, 2)),
                columns=list('AB'))
print(A)
```

```{python}
#| echo: true
B = pd.DataFrame(rng.randint(0, 10, (3, 3)),
                columns=list('BAC'))
print(B)
```

## DataFrame Alignment

Addition with automatic alignment

```{python}
A + B
```

Fill values can be specified for missing entries.

```{python}
#| echo: true
fill = A.stack().mean()
# Fill with mean of all values in A
A.add(B, fill_value=fill)
```

## Operations Between DataFrame and Series

Operations between DataFrames and Series maintain proper alignment.

```{python}
#| echo: true
df = pd.DataFrame(rng.randint(10, size=(3, 4)),
                 columns=list('QRST'))
print(df)
```

## Operations Between DataFrame and Series

```{python}
#| echo: true
# Row-wise operation (default)
df - df.iloc[0]
```

```{python}
#| echo: true
# Column-wise operation
df.subtract(df['R'], axis=0)
```

## Operations Between DataFrame and Series

Partial Series operations also align correctly:

```{python}
#| echo: true
# Select every other column from first row
halfrow = df.iloc[0, ::2]  
print(halfrow)
```

```{python}
#| echo: true
# Subtraction only affects matching columns
df - halfrow 
```

## Pandas Method Equivalents

| Python Operator | Pandas Method(s)                 |
|-----------------|----------------------------------|
| `+`             | `add()`                          |
| `-`             | `sub()`, `subtract()`            |
| `*`             | `mul()`, `multiply()`            |
| `/`             | `truediv()`, `div()`, `divide()` |
| `//`            | `floordiv()`                     |
| `%`             | `mod()`                          |
| `**`            | `pow()`                          |

## Pandas Method Equivalents

-   `+` `add()`
-   `-` `sub()`, `subtract()`
-   `*` `mul()`, `multiply()`
-   `/` `truediv()`, `div()`, `divide()`
-   `//` `floordiv()`
-   `%` `mod()`
-   `**` `pow()`

## Hands-on - UFunc

```{python}
#| echo: true
# Create a DataFrame with COVID-19 data for the last 5 days
dates = pd.date_range(end='2023-04-30', periods=5)
data = {
    'United States': [45000, 42000, 38000, 41000, 39000],
    'India': [12000, 13500, 14200, 13800, 12500],
    'Brazil': [9800, 9200, 8700, 9100, 9500],
    'France': [7500, 7200, 6800, 6500, 6200],
    'Germany': [5200, 4800, 4600, 4900, 5100]
}

daily_cases = pd.DataFrame(data, index=dates)
daily_cases.index.name = 'Date'
daily_cases.columns.name = 'Country'
print("Daily New COVID-19 Cases:")
print(daily_cases)
```

## Hands-on - UFunc

-   Create a DataFrame showing the percentage change from the first day for each country
-   Subtract the mean daily cases for each country from the daily cases DataFrame
-   Calculate the ratio of daily cases in the United States compared to each other country
-   Create a new DataFrame showing the 3-day moving average of cases for each country

# Handling Missing Data in Pandas

## Missing Data Representations

Pandas uses two main sentinel values to represent missing data:

1.  **None**: A Python singleton object
    -   Can only be used in arrays with `dtype='object'`
    -   Operations on arrays with `None` values are performed at the Python level (slower)
    -   Aggregations like `sum()` or `min()` will generally result in errors
2.  **NaN (Not a Number)**: A special floating-point value
    -   Part of the IEEE floating-point specification
    -   Arrays with NaN maintain native data types (faster operations)
    -   NaN "infects" other values in arithmetic operations
    -   Aggregations return NaN unless using special functions like `np.nansum()`

## Detecting Null Values

```{python}
#| echo: true
# Generate boolean mask of null values
data.isnull()
data.notnull()  # Opposite of isnull()
```

```{python}
#| echo: true
# Use as a mask
data[data.notnull()]
```

## Dropping Null Values

```{python}
#| echo: true
# For Series - straightforward removal
data.dropna()
```

```{python}
#| echo: true
# Drop rows with any null values
df.dropna()                     
```

```{python}
#| echo: true
# Drop columns with any null values
df.dropna(axis='columns')       
```

## Dropping Null Values

```{python}
#| echo: true
# Drop rows with all null values
df.dropna(how='all')            
```

```{python}
#| echo: true
# Drop columns with all null values
df.dropna(axis=1, how='all')    
```

```{python}
#| echo: true
# Keep rows with at least 3 non-null values
df.dropna(thresh=3)             
```

## Filling Null Values

```{python}
#| echo: true
# Fill with a constant value
data.fillna(0)
```

```{python}
#| echo: true
# Forward-fill (propagate last valid value)
data.fillna(method='ffill')
```

## Filling Null Values

```{python}
#| echo: true
# Back-fill (use next valid value)
data.fillna(method='bfill')
```

```{python}
#| echo: true
# For DataFrames, can specify axis
df.fillna(method='ffill', axis=1)
```

## Hands-on - Missing Value

```{python}
#| echo: true
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Create a DataFrame with COVID-19 data (with some missing values)
data = {
    'Country': ['USA', 'India', 'Brazil', 'France', 'Germany', 'UK', 'Italy', 'Spain', 'Russia', 'Mexico'],
    'Cases': [98200000, 44700000, 37600000, 35400000, 30200000, np.nan, 25000000, 13800000, np.nan, 7300000],
    'Deaths': [1100000, 530000, 690000, 160000, 170000, 220000, np.nan, 120000, 400000, np.nan],
    'Recovered': [93100000, 43200000, 35900000, 34800000, 29500000, 24100000, None, 13600000, 18200000, 6900000],
    'Tests': [np.nan, 900000000, 180000000, 271000000, 122000000, 522000000, 265000000, 471000000, np.nan, 18000000]
}

covid_df = pd.DataFrame(data)
print("COVID-19 Dataset:")
print(covid_df)
```

## Hands-on - Missing Value

-   Check for missing values in the dataset using isnull() and calculate the sum for each column
-   Drop only rows where the 'Deaths' column has missing values
-   Use forward fill (ffill) to propagate the last valid observation forward
-   Create a new column 'Case_Fatality_Rate' calculated as Deaths/Cases, handling any resulting NaN values appropriately

# Hierarchical Indexing in Pandas

## Introduction to Multi-Indexing

-   Hierarchical indexing (multi-indexing)
-   Represent higher-dimensional data within the familiar one-dimensional Series and two-dimensional DataFrame objects.
-   Incorporate multiple index levels within a single index, providing a powerful way to work with complex data structures.

## Creating a Multi-Indexed Series

Create a multi-indexed Series by using tuples as keys.

```{python}
#| echo: true
index = [('California', 2000), ('California', 2010),
         ('New York', 2000), ('New York', 2010),
         ('Texas', 2000), ('Texas', 2010)]
populations = [33871648, 37253956, 18976457, 19378102, 20851820, 25145561]
pop = pd.Series(populations, index=index)
print(pop)
```

## The Better Way: MultiIndex

A more efficient approach is to use Pandas' MultiIndex.

```{python}
#| echo: true
# Create MultiIndex from tuples
index = pd.MultiIndex.from_tuples(index)
print(index)
```

```{python}
#| echo: true
pop = pop.reindex(index)
print(pop)
```

## The Better Way: MultiIndex

With this structure, we can easily select data using intuitive slicing.

```{python}
#| echo: true
# Select all data from 2010
pop[:, 2010]
```

## MultiIndex Construction Methods

There are several ways to create a MultiIndex.

```{python}
#| echo: true
# From arrays
pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'], [1, 2, 1, 2]])
```

```{python}
#| echo: true
# From tuples
pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])
```

## MultiIndex Construction Methods

There are several ways to create a MultiIndex.

```{python}
#| echo: true
# From Cartesian product
pd.MultiIndex.from_product([['a', 'b'], [1, 2]])
```

```{python}
#| echo: true
# Direct construction
pd.MultiIndex(levels=[['a', 'b'], [1, 2]], 
              labels=[[0, 0, 1, 1], [0, 1, 0, 1]])
```

## Naming Levels

You can name the levels for better clarity.

```{python}
#| echo: true
pop.index.names = ['state', 'year']
print(pop)
```

## MultiIndex for Columns

DataFrames can have multi-indexed columns as well:

```{python}
#| echo: true
# Create hierarchical indices and columns
index = pd.MultiIndex.from_product([[2013, 2014], [1, 2]], names=['year', 'visit'])
columns = pd.MultiIndex.from_product([['Bob', 'Guido', 'Sue'], ['HR', 'Temp']], names=['subject', 'type'])

health_data = pd.DataFrame(data, index=index, columns=columns)
print(health_data)
```

## Indexing and Slicing

```{python}
#| echo: true
# Access single elements
pop['California', 2000]
```

```{python}
#| echo: true
# Partial indexing (returns a Series)
pop['California']
```

```{python}
#| echo: true
# Slicing between states
pop.loc['California':'New York']
```

```{python}
#| echo: true
# Selecting specific level
pop[:, 2000]
```

```{python}
#| echo: true
# Boolean masking
pop[pop > 22000000]
```

```{python}
#| echo: true
# Fancy indexing
pop[['California', 'Texas']]
```

## Indexing and Slicing - multi-indexed columns

```{python}
#| echo: true
# Select a specific person's data
health_data['Guido']
```

```{python}
#| echo: true
# Select specific measurements
health_data['Guido', 'HR']
```

```{python}
#| echo: true
# Use IndexSlice for complex selections
idx = pd.IndexSlice
health_data.loc[idx[:, 1], idx[:, 'HR']]
```

## Sorting Indices

MultiIndex operations often require sorted indices:

```{python}
#| echo: true
# Sort index for proper slicing
print(data)
data_sort = data.sort_index()
print(data_sort)
```

## Stacking and Unstacking

```{python}
#| echo: true
# Convert Series to DataFrame
print(pop)
pop_df = pop.unstack()
print(pop_df)
```

## Index Manipulation

```{python}
#| echo: true
# Convert index to columns
pop_flat = pop.reset_index(name='population')
print(pop_flat)
```

```{python}
#| echo: true
# Convert columns to index
pop_flat.set_index(['state', 'year'])
```

## Data Aggregations

```{python}
#| echo: true
print(health_data)
```

```{python}
#| echo: true
# Average by year
data_mean = health_data.mean(level='year')
print(data_mean)
```

```{python}
#| echo: true
# Average by measurement type across columns
data_mean.mean(axis=1, level='type')
```

## Hands-on - Hierarchical Indexing

```{python}
#| echo: true
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# Create a healthcare dataset with patient information across different hospitals and departments
# Row index: Hospital, Department, Patient ID
# Column index: Metric Type, Measurement

# Create sample data
hospitals = ['General Hospital', 'General Hospital', 'Community Clinic', 'Community Clinic']
departments = ['Cardiology', 'Neurology'] * 2
patient_ids = list(range(1001, 1009))  # 8 patients

# Create row multi-index
row_tuples = [(h, d, p) for h, d in zip(hospitals * 2, departments * 2) for p in [patient_ids.pop(0)]]
row_index = pd.MultiIndex.from_tuples(row_tuples, names=['Hospital', 'Department', 'PatientID'])

# Create column multi-index
metric_types = ['Vital Signs', 'Vital Signs', 'Lab Results', 'Lab Results']
measurements = ['Blood Pressure', 'Heart Rate', 'Glucose', 'Cholesterol']
col_index = pd.MultiIndex.from_arrays([metric_types, measurements], names=['Type', 'Measurement'])

# Generate sample data
np.random.seed(42)
data = np.random.randint(60, 180, size=(8, 4))
health_df = pd.DataFrame(data, index=row_index, columns=col_index)
print("Healthcare Patient Data:")
print(health_df)
```

## Hands-on - Hierarchical Indexing

-   Use IndexSlice to select all vital signs for patients in the Cardiology department
-   Calculate the average values for each measurement by hospital
-   Find the patient with the highest blood pressure in each department
-   Create a boolean mask to identify patients with heart rate \> 100 and glucose \> 120
-   Reset the index to convert the hierarchical index to columns, then set it back again
-   Create a pivot table showing the average measurements by hospital and department

# Combining Datasets: Concat and Append

## Helper Function

```{python}
#| echo: true
def make_df(cols, ind):
    """Quickly make a DataFrame"""
    data = {c: [str(c) + str(i) for i in ind]
            for c in cols}
    return pd.DataFrame(data, ind)

class display(object):
    """Display HTML representation of multiple objects"""
    template = """<div style="float: left; padding: 10px;">
    <p style='font-family:"Courier New", Courier, monospace'>{0}</p>{1}
    </div>"""
    def __init__(self, *args):
        self.args = args
        
    def _repr_html_(self):
        return '\n'.join(self.template.format(a, eval(a)._repr_html_())
                         for a in self.args)
    
    def __repr__(self):
        return '\n\n'.join(a + '\n' + repr(eval(a))
                           for a in self.args)
```

## Basic Concatenation - Series

pandas offers `pd.concat()` for combining data

```{python}
#| echo: true
import pandas as pd
import numpy as np
ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])
ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])
pd.concat([ser1, ser2])
```

## Basic Concatenation - DataFrames

Default: Concatenate vertically (axis=0)

```{python}
#| echo: true
df1 = make_df('AB', [1, 2]) 
df2 = make_df('AB', [3, 4])  
print(df1)
print(df2)

pd.concat([df1, df2])
```

## Concatenation - Axis Selection

You can concatenate column-wise by `axis=1`

```{python}
#| echo: true
df3 = make_df('AB', [0, 1])
df4 = make_df('CD', [0, 1])
df5 = pd.concat([df3, df4], axis=1) 
display('df3', 'df4', 'df5')
```

## Concatenation - Duplicate Indices

-   Pandas preserves indices during concatenation
    -   can result in duplicate indices

```{python}
#| echo: true
x = make_df('AB', [0, 1])
y = make_df('AB', [0, 1])
display('x', 'y', 'pd.concat([x, y])')
```

## Concatenation - Handling Duplicate Indices

1.  **Verify integrity**: Raise an error if duplicates exist

```{python}
#| echo: true
try:
    pd.concat([x, y], verify_integrity=True)
except ValueError as e:
    print("ValueError:", e)
```

2.  **Ignore indices**: Create new integer indices

```{python}
#| echo: true
index2 = pd.concat([x, y], ignore_index=True)
display('x', 'y', "index2")
```

3.  **Add hierarchical keys**: Create a MultiIndex to distinguish sources

```{python}
#| echo: true
index3 = pd.concat([x, y], keys=['x', 'y'])
display('x', 'y', "index3")
```

## Concatenation - Join Behavior

When concatenating DataFrames with shared columns - **Join**

```{python}
#| echo: true
df5 = make_df('ABC', [1, 2])
df6 = make_df('BCD', [3, 4])
display('df5', 'df6', 'pd.concat([df5, df6])')
```

## Concatenation - Join Behavior

1.  **Outer join** (default): Union of columns, filling missing values with NaN

```{python}
#| echo: true
# join='outer' is default
outdf = pd.concat([df5, df6])  
display('df5', 'df6', 'outdf')
```

2.  **Inner join**: Only keep columns present in all inputs

```{python}
#| echo: true
inndf = pd.concat([df5, df6], join='inner')
display('df5', 'df6', 'inndf')
```

## Hands-on - Concat and Append

In this exercise, you'll work with sales data from different regional offices that need to be combined for company-wide analysis.

```{python}
#| echo: true
import pandas as pd
import numpy as np

# Create sales data for different regions
def create_sales_data(region, periods, seed=None):
    if seed is not None:
        np.random.seed(seed)
    
    dates = pd.date_range(start='2025-01-01', periods=periods)
    products = ['Laptop', 'Phone', 'Tablet', 'Monitor']
    
    data = {
        'Date': np.random.choice(dates, size=periods),
        'Product': np.random.choice(products, size=periods),
        'Units': np.random.randint(1, 50, size=periods),
        'Price': np.random.uniform(100, 1500, size=periods).round(2),
        'Region': region
    }
    
    df = pd.DataFrame(data)
    df['Revenue'] = df['Units'] * df['Price']
    return df

# Create regional datasets
north_sales = create_sales_data('North', 20, seed=42)
south_sales = create_sales_data('South', 15, seed=43)
east_sales = create_sales_data('East', 25, seed=44)
west_sales = create_sales_data('West', 18, seed=45)

# Display the first few rows of each dataset
print("North Region Sales:")
print(north_sales.head(3))
print("\nSouth Region Sales:")
print(south_sales.head(3))
```

## Hands-on - Concat and Append

-   Combine all regional sales data into a single DataFrame
-   Reset indices as 'Date' in original DataFrames, then perfrom default concatenation (preserves original indices)
-   When perform concatenation, adding hierarchical keys to track source

# Combining Datasets: Merge and Join

## Helper Function

```{python}
#| echo: true
class display(object):
    """Display HTML representation of multiple objects"""
    template = """<div style="float: left; padding: 10px;">
    <p style='font-family:"Courier New", Courier, monospace'>{0}</p>{1}
    </div>"""
    def __init__(self, *args):
        self.args = args
    def _repr_html_(self):
        return '\n'.join(self.template.format(a, eval(a)._repr_html_())
                         for a in self.args)
    def __repr__(self):
        return '\n\n'.join(a + '\n' + repr(eval(a))
                           for a in self.args)
```

## Relational Algebra in Pandas

-   Pandas provides high-performance, in-memory join and merge operations through the `pd.merge` function

1.  **One-to-one joins**: Combine datasets with unique keys in both DataFrames
2.  **Many-to-one joins**: Join when one DataFrame has duplicate keys
3.  **Many-to-many joins**: Join when both DataFrames have duplicate keys

## Basic Join Example

The result combines information from both DataFrames using the common column as the key.

```{python}
#| echo: true
df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})
df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],'hire_date': [2004, 2008, 2012, 2014]})
dfmerge = pd.merge(df1, df2)
display('df1','df2','dfmerge')
```

## Specifying Merge Keys

Using the `on` Parameter

```{python}
#| echo: true
pd.merge(df1, df2, on='employee')
```

## Different Column Names

`left_on` and `right_on` Parameter

```{python}
#| echo: true
df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],'salary': [70000, 80000, 120000, 90000]})
pd.merge(df1, df3, left_on="employee", right_on="name")
```

## Merging on Index

`left_index` and `right_index` Parameter

```{python}
#| echo: true
df1a = df1.set_index('employee')
df2a = df2.set_index('employee')
pd.merge(df1a, df2a, left_index=True, right_index=True)
```

The DataFrame `.join()` method provides a shortcut for index-based merges:

```{python}
#| echo: true
df1a.join(df2a)
```

## Join Types

`how` Parameter

-   **Inner join** (default): Returns only matching rows

```{python}
#| echo: true
df7 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],
                    'food': ['fish', 'beans', 'bread']},
                   columns=['name', 'food'])
df8 = pd.DataFrame({'name': ['Mary', 'Joseph'],
                    'drink': ['wine', 'beer']},
                   columns=['name', 'drink'])
innerpd = pd.merge(df7, df8, how='inner')
display(df7, df8, innerpd)
```

-   **Outer join**: Returns all rows from both DataFrames, filling missing values with NaN

```{python}
#| echo: true
outerpd = pd.merge(df7, df8, how='outer')
display(df7, df8, outerpd)
```

## Join Types

-   **Left join**: Returns all rows from the left DataFrame

```{python}
#| echo: true
leftpd = pd.merge(df7, df8, how='left')
display(df7, df8, leftpd)
```

-   **Right join**: Returns all rows from the right DataFrame

```{python}
#| echo: true
rightpd = pd.merge(df7, df8, how='right')
display(df7, df8, rightpd)
```

## Handling Duplicate Column Names

When DataFrames have conflicting column names, Pandas adds suffixes:

```{python}
#| echo: true
# Default suffixes (_x and _y)
df9 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
                    'rank': [1, 2, 3, 4]})
df10 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
                    'rank': [3, 1, 4, 2]})
pd.merge(df9, df10, on="name")
```

```{python}
#| echo: true
# Custom suffixes
pd.merge(df9, df10, on="name", suffixes=["_L", "_R"])
```

## Hands-on: US States Data

```{python}
#| echo: true
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
pop_url = "https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-population.csv"
area_url = "https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-areas.csv"
abb_url = "https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-abbrevs.csv"
```

## Hands-on: US States Data

```{python}
#| echo: true
pop = pd.read_csv(pop_url)
areas = pd.read_csv(area_url)
abbrevs = pd.read_csv(abb_url)

display('pop.head()', 'areas.head()', 'abbrevs.head()')
```

## Hands-on: US States Data

Merge multiple datasets to calculate population density:

1.  Merge population data with state abbreviations
2.  Handle missing values and mismatches
3.  Merge with area data
4.  Calculate and rank US states and territories by their 2010 population density

## Hint 1 `.query()`

```{python}
#| echo: true
pop2010 = pop.query("year == 2010 & ages == 'total'")
pop2010.head()
```

## Hint 2 PR

```{python}
#| echo: true
pop2010.query("`state/region`	 == 'PR'")
```

Do we have PR in abbrevs table?

```{python}
#| echo: true
PRabb = abbrevs.query("abbreviation == 'PR'")
PRabb
```

## Hint 3 Sort

`.sort_values(ascending=False)`

# Aggregation and Grouping in Pandas

## Basic Aggregation

Pandas provides powerful tools for summarizing large datasets

For Series objects, aggregations return a single value

```{python}
#| echo: true
ser = pd.Series([0.37, 0.95, 0.73, 0.60, 0.16])
ser.sum()    
ser.mean()  
```

## Basic Aggregation

For DataFrames, aggregations operate on each column by

```{python}
#| echo: true
df = pd.DataFrame({'A': [0.16, 0.06, 0.87, 0.60, 0.71],
                   'B': [0.02, 0.97, 0.83, 0.21, 0.18]})
df.mean()
```

Use axis parameter to aggregate across rows instead

```{python}
#| echo: true
df.mean(axis='columns')
```

## Basic Aggregation

quick statistical summary of your data: `describe()`

```{python}
#| echo: true
planets = sns.load_dataset('planets')
planets.dropna().describe()
```

## GroupBy: Split, Apply, Combine

`.groupby()` for more sophisticated data analysis by:

1.  **Splitting** data into groups based on some criteria

2.  **Applying** a function to each group independently

3.  **Combining** the results into a data structure

![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/03.08-split-apply-combine.png)

## GroupBy: Split, Apply, Combine

```{python}
#| echo: true
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],
                   'data': range(6)})
df.groupby('key').sum()
```

## Working with GroupBy Objects

```{python}
#| echo: true
planets.groupby('method')
```

```{python}
#| echo: true
planets.groupby('method')['orbital_period']
```

## Working with GroupBy Objects

**Column Selection**: Select specific columns from groups

```{python}
#| echo: true
planets.groupby('method')['orbital_period'].median()
```

**Iteration**: Iterate through groups

```{python}
#| echo: true
for method, group in planets.groupby('method'):
    print(f"{method:30s} shape={group.shape}")
```

## Working with GroupBy Objects

**Dispatch Methods**: Apply DataFrame methods to each group

```{python}
#| echo: true
planets.groupby('method')['year'].describe()
```
## Working with GroupBy Objects

`.unstack()`

```{python}
#| echo: true
planets.groupby('method')['year'].describe().unstack()
```

## Advanced GroupBy Operations
```{python}
#| echo: true
rng = np.random.RandomState(0)
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],
                   'data1': range(6),
                   'data2': rng.randint(0, 10, 6)},
                   columns = ['key', 'data1', 'data2'])
df
```

## Advanced GroupBy Operations

**Aggregation**: Multiple aggregations at once. It can take a string, a function, or a list thereof

```{python}
#| echo: true
df.groupby('key').aggregate(['min', np.median, 'max'])
```

Different aggregations per column

```{python}
#| echo: true
df.groupby('key').aggregate({'data1': 'min', 'data2': 'max'})
```

## Advanced GroupBy Operations

**Filtering**: Keep only groups that satisfy a condition

```{python}
#| echo: true
def filter_func(x):
    return x['data2'].std() > 4

display('df', "df.groupby('key').std()", "df.groupby('key').filter(filter_func)")
```

## Advanced GroupBy Operations

**Transformation**: Apply a transformation that preserves the input shape

```{python}
#| echo: true
df.groupby('key').transform(lambda x: x - x.mean())
```

## Advanced GroupBy Operations

**Apply**: Apply arbitrary functions to each group

```{python}
#| echo: true
def norm_by_data2(x):
    # x is a DataFrame of group values
    x['data1'] /= x['data2'].sum()
    return x

display('df', "df.groupby('key').apply(norm_by_data2)")
```

## Flexible Grouping

Groups can be specified in multiple ways:

1.  By column name: `df.groupby('key')`
2.  By list/array: `df.groupby([1,1,2])`
3.  By dictionary mapping: `df2.groupby({'A': 'vowel', 'B': 'consonant'})`
4.  By function: `df2.groupby(str.lower)`
5.  By multiple keys: `df.groupby(['key1', 'key2'])`

## Practical Example

Analyzing planet discovery methods by decade

```{python}
#| echo: true
decade = 10 * (planets['year'] // 10)
decade = decade.astype(str) + 's'
decade.name = 'decade'
planets.groupby(['method', decade])['number'].sum()
```

## Practical Example

```{python}
planets.groupby(['method', decade])['number'].sum().unstack().fillna(0)
```

## Pivot Tables in Pandas

Pivot tables are a powerful data analysis tool that provide multidimensional summarization of data. They can be thought of as a multidimensional version of GroupBy aggregation.

### Basic Syntax

```{python}
#| echo: true
df.pivot_table(values, index, columns, aggfunc='mean')
```

### Key Features

-   Transforms column-wise data into a two-dimensional table
-   Allows for multiple levels of indices and columns
-   Supports various aggregation functions

### Example: Titanic Dataset

```{python}
#| echo: true
import pandas as pd
import seaborn as sns

titanic = sns.load_dataset('titanic')

# Basic pivot table
survival_by_class = titanic.pivot_table('survived', index='sex', columns='class')

# Multi-level pivot table
age_groups = pd.cut(titanic['age'], [0, 18, 80])
survival_by_age_class = titanic.pivot_table('survived', ['sex', age_groups], 'class')
```

### Additional Options

-   `aggfunc`: Specify aggregation function (e.g., 'sum', 'mean', 'count')
-   `fill_value`: Replace missing values
-   `margins`: Add row/column totals
-   `dropna`: Remove rows with missing values

### Practical Application: US Birth Data

```{python}
#| echo: true
births = pd.read_csv('births.csv')
births['decade'] = 10 * (births['year'] // 10)

# Analyze births by decade and gender
births_by_decade = births.pivot_table('births', index='decade', columns='gender', aggfunc='sum')

# Visualize trends
births_by_decade.plot()
```

Pivot tables in Pandas provide a flexible and efficient way to analyze complex datasets, revealing patterns and trends that might otherwise be difficult to discern.

## Working with Strings in Pandas

Pandas provides powerful vectorized string operations for efficient data cleaning and manipulation. These operations are accessible through the `str` attribute of Series and Index objects containing strings.

### Key Features

-   Vectorized operations for speed and efficiency
-   Handles missing values automatically
-   Supports regular expressions

### Common String Methods

Pandas implements most Python string methods as vectorized operations:

-   `lower()`, `upper()`, `capitalize()`: Change case
-   `len()`: Get string lengths
-   `startswith()`, `endswith()`: Check string prefixes/suffixes
-   `split()`: Split strings into lists

### Regular Expression Methods

-   `extract()`: Extract matched groups
-   `findall()`: Find all occurrences of a pattern
-   `replace()`: Replace occurrences of a pattern
-   `contains()`: Check if pattern exists in string

### Specialized Methods

-   `get()`, `slice()`: Access individual characters or substrings
-   `cat()`: Concatenate strings
-   `get_dummies()`: Create indicator variables from categorical data

### Example: Recipe Database

A practical application demonstrates using these string methods to clean and analyze a messy dataset of recipes:

1.  Load and parse JSON data
2.  Explore ingredient lists using string methods
3.  Create a simple recipe recommender based on ingredients

This example showcases how Pandas string operations can efficiently handle real-world data cleaning tasks, which often comprise a significant portion of data science work.

# Working with Time Series in Pandas

## Types of Time Data

Pandas provides extensive tools for working with time-based data, reflecting its origins in financial modeling:

-   **Time stamps**: Reference specific moments (e.g., July 4th, 2015 at 7:00am)
-   **Time intervals/periods**: Reference lengths of time between points (e.g., the year 2015)
-   **Time deltas/durations**: Reference exact time lengths (e.g., 22.56 seconds)

## Date and Time Objects in Python

### Native Python (datetime and dateutil)

```{python}
#| echo: true
from datetime import datetime
datetime(year=2015, month=7, day=4)

from dateutil import parser
date = parser.parse("4th of July, 2015")
date.strftime('%A')  # 'Saturday'
```

### NumPy's datetime64

```{python}
#| echo: true
import numpy as np
date = np.array('2015-07-04', dtype=np.datetime64)
date + np.arange(12)  # Array of consecutive dates
```

NumPy's datetime64 uses a fundamental time unit that creates a trade-off between resolution and time span:

| Code | Meaning    | Time span (relative) |
|------|------------|----------------------|
| `Y`  | Year       | ± 9.2e18 years       |
| `D`  | Day        | ± 2.5e16 years       |
| `ns` | Nanosecond | ± 292 years          |

### Pandas Time Objects

Pandas combines the ease-of-use of datetime with the efficiency of numpy.datetime64:

```{python}
#| echo: true
import pandas as pd
date = pd.to_datetime("4th of July, 2015")
date.strftime('%A')  # 'Saturday'
```

## Pandas Time Series Data Structures

-   **Timestamp**: Replacement for Python's datetime
-   **DatetimeIndex**: Index structure for timestamps
-   **Period/PeriodIndex**: For fixed-frequency time intervals
-   **Timedelta/TimedeltaIndex**: For time durations

### Creating Date Ranges

```{python}
#| echo: true
# Daily frequency (default)
pd.date_range('2015-07-03', '2015-07-10')

# Hourly frequency
pd.date_range('2015-07-03', periods=8, freq='H')

# Monthly periods
pd.period_range('2015-07', periods=8, freq='M')
```

## Time Series Indexing

```{python}
#| echo: true
# Create time-indexed Series
index = pd.DatetimeIndex(['2014-07-04', '2014-08-04', 
                          '2015-07-04', '2015-08-04'])
data = pd.Series([0, 1, 2, 3], index=index)

# Slice by date range
data['2014-07-04':'2015-07-04']

# Slice by year
data['2015']
```

## Frequency and Offset Codes

Common frequency codes: - `D`: Calendar day - `B`: Business day - `W`: Weekly - `M`: Month end - `Q`: Quarter end - `A`: Year end - `H`: Hours - `T`: Minutes

## Time Series Operations

### Resampling

```{python}
#| echo: true
# Downsample to annual frequency
goog.resample('BA').mean()  # Mean for each year
goog.asfreq('BA')  # Value at end of each year
```

### Time-shifting

```{python}
#| echo: true
# Shift data values
goog.shift(900)

# Shift index values
goog.tshift(900)
```

### Rolling Windows

```{python}
#| echo: true
# One-year centered rolling statistics
rolling = goog.rolling(365, center=True)
rolling.mean()  # Rolling mean
rolling.std()   # Rolling standard deviation
```

These powerful time series tools enable efficient analysis of temporal data, from simple date manipulations to complex financial modeling.
