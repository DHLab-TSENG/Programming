---
title: "6. Pandas"
author: "Yi-Ju Tseng"
format:
  revealjs:
    slide-number: c/t
    show-slide-number: all
editor: visual
---

## Pandas

-   Introduction to Pandas Objects
-   Data Indexing and Selection in Pandas
-   Operations in Pandas
-   Handling Missing Data in Pandas
-   Hierarchical Indexing in Pandas
-   Combining Datasets: Concat and Append
-   Combining Datasets: Merge and Join
-   Aggregation and Grouping in Pandas
-   Working with Time Series in Pandas

# Introduction to Pandas Objects

## Introduction to Pandas Objects

-   Pandas objects are enhanced versions of NumPy arrays
    -   we need more flexibility (e.g., attaching labels to data, working with missing data, etc.)
-   Rows and columns identified with **labels** instead of integer indices
-   Three fundamental data structures:
    -   Series
    -   DataFrame
    -   Index

## Pandas Getting Started

Install

```{python}
#| eval: false
#| echo: true
!pip3 install numpy pandas # ! for shell script
```

Import

```{python}
#| echo: true
import numpy as np
import pandas as pd
```

## The Pandas Series Object

-   One-dimensional array of indexed data
-   Wraps both values and indices
-   `pd.Series(np 1D array or list)`
    -   From list (default integer index)
    -   With custom index
    -   From dictionary

```{python}
#| echo: true
data = pd.Series([0.25, 0.5, 0.75, 1.0])
```

## Series: Data Access
```{python}
#| echo: true
data
```
-   Position (index)
-   Slicing
```{python}
#| echo: true
# Access by position
data[1] 
```
```{python}
#| echo: true
# Slicing works too
data[1:3]    
```

## Series: Flexible Indices

-   Pandas Series has an **defined index** associated with the values
-   Can use strings or other types

```{python}
#| echo: true
data = pd.Series([0.25, 0.5, 0.75, 1.0],
                 index=['a', 'b', 'c', 'd'])
print(data)
data['b']
```

## Series as Specialized Dictionary

-   **keys**: index
-   **values**: values

```{python}
#| echo: true
population_dict = {'California': 38332521,
                   'Texas': 26448193,
                   'New York': 19651127,
                   'Florida': 19552860,
                   'Illinois': 12882135}
population = pd.Series(population_dict)
print(population)
```

## Series: Dictionary-like Operations
```{python}
#| echo: true
population
```
Index (Dictionary-style access)
```{python}
#| echo: true
population['California'] 
```
Slicing! (Array-style operations) **Includes the end point**
```{python}
#| echo: true
population['California':'New York']
```

## Hands-on - Series

-   Create a Series containing the following student scores (out of 100): 85, 92, 78, 90, 88, and use student names as the index: 'Alice', 'Bob', 'Charlie', 'David', 'Emma'.
-   Series Operations
    -   Find Bob's score
    -   Find the average score
    -   Find all scores above 85 (`Hint`: Boolean)
    -   Add 5 points to everyone's score  (`Hint`: +)

## The Pandas DataFrame Object

-   Two-dimensional data structure
    -   with both flexible **row indices** and flexible **column names**
-   Can be viewed as:
    -   Generalized NumPy array
    -   Specialization of Python dictionary
    -   Collection of aligned Series objects

## DataFrame Construction Methods

-   From a single Series
-   From a two-dimensional NumPy array
-   From a dictionary of Series

## DataFrame: From a Single Series

```{python}
pd.DataFrame(population, columns=['population'])
```

## DataFrame: From 2D NumPy Array

```{python}
#| echo: true
np.random.rand(3, 2)
```

```{python}
#| echo: true
pd.DataFrame(np.random.rand(3, 2),
             columns=['foo', 'bar'],
             index=['a', 'b', 'c'])
```

## DataFrame: From a Dictionary of Series

```{python}
#| echo: true
# Create area Series
area_dict = {'California': 423967, 'Texas': 695662, 
             'New York': 141297, 'Florida': 170312, 
             'Illinois': 149995}
area = pd.Series(area_dict)
# Combine with population Series
states = pd.DataFrame({'population': population,
                       'area': area})
print(states)
```

## DataFrame: Structure

Access index (row labels)
```{python}
#| echo: true
print(states.index)
```
Access columns

```{python}
#| echo: true
print(states.columns)
```

## DataFrame as Dictionary of Series

Access a column values (returns a Series) with `column name` and `[]`

```{python}
#| echo: true
states['area']
```


## The Pandas Index Object

-   Used by both Series and DataFrame
-   Can be viewed as:
    -   Immutable array
    -   Ordered set

```{python}
#| echo: true
ind = pd.Index([2, 3, 5, 7, 11])
print(ind)
```

## Summary

-   **Series**: 1D array with labeled index
-   **DataFrame**: 2D table with labeled rows and columns
-   **Index**: Immutable array-like structure for labeling

## Hands-on - DataFrame

You have information about countries stored in dictionaries. Convert this to a DataFrame `countries_df`.

```{python}
#| echo: true     
# Midyear Population
population_dict = {
    'USA': 331002651,
    'China': 1439323776,
    'India': 1380004385,
    'Brazil': 212559417,
    'Japan': 126476461
}
# Area (square km)
area_dict = {
    'USA': 9833517,
    'China': 9596960,
    'India': 3287263,
    'Brazil': 8515767,
    'Japan': 377975
}
#GDP
gdp_dict = {
    'USA': 21433225,
    'China': 14342903,
    'India': 2875142,
    'Brazil': 1839758,
    'Japan': 5081770
}
```

## Hands-on - DataFrame

-   Add the following columns to the `countries_df` (`Hint`: like numpy 2D array):
    -   Population density (population per square km)
    -   GDP per capita (gross domestic product divided by midyear population)

# Data Indexing and Selection in Pandas

## Series Indexing - 1/2
```{python}
#| echo: true
data = pd.Series([0.25, 0.5, 0.75, 1.0],
                 index=['a', 'b', 'c', 'd'])
print(data)
```
**Dictionary-style indexing**: Access values by index (labels)

```{python}
#| echo: true
data['b']  
```

**Attribute-style access**: For string index (labels)

```{python}
#| echo: true
data.b  # Same as data['b']
```

## Series Indexing - 2/2

**Slicing with explicit index**: Includes the end point

```{python}
#| echo: true
data['a':'c']  # Includes 'a', 'b', 'c'
```

**Slicing with implicit integer index**: Excludes the end point

```{python}
#| echo: true
data[0:2]  # Includes indices 0, 1
```

## Boolean masking

**Boolean masking**: Filter based on conditions

```{python}
#| echo: true
data[(data > 0.3) & (data < 0.8)]   
```

## Special Indexers - loc

Pandas provides special indexers to avoid confusion with integer indices:
-   **loc**: **Label-based** indexing
-   **iloc**: Integer-based indexing

```{python}
#| echo: true
data_int_index = pd.Series([0.25, 0.5, 0.75, 1.0],
                 index=[1, 2, 3, 4]) ## a bit confusing
print(data_int_index)
```
**loc**: **Label-based** indexing
```{python}
#| echo: true
data_int_index.loc[1]  
```

## Special Indexers - iloc
```{python}
#| echo: true
print(data_int_index)
```

**iloc**: **Integer**-based indexing

```{python}
#| echo: true
data_int_index.iloc[1]  
```

## DataFrame Indexing

```{python}
#| echo: true
area = pd.Series({'California': 423967, 'Texas': 695662,
                  'New York': 141297, 'Florida': 170312,
                  'Illinois': 149995})
pop = pd.Series({'California': 38332521, 'Texas': 26448193,
                 'New York': 19651127, 'Florida': 19552860,
                 'Illinois': 12882135})
df = pd.DataFrame({'area':area, 'pop':pop})
print(df)
```

## DataFrame Access
```{python}
#| echo: true
print(df)
```
**Column access**

```{python}
#| echo: true
df['area']  # Dictionary-style
```
```{python}
#| echo: true
df.area     # Attribute-style
```

## DataFrame Access
```{python}
#| echo: true
print(df)
```
**Adding new columns**

```{python}
  #| echo: true
df['density'] = df['pop'] / df['area']
print(df)
```

## Special Indexers
```{python}
#| echo: true
print(df)
```
**loc**: Label-based indexing

```{python}
#| echo: true
print(df.loc[:'Illinois',:'pop'])
```

**iloc**: Integer-based indexing

```{python}
#| echo: true
print(df.iloc[:3, :2])
```

## Additional Indexing Conventions
```{python}
#| echo: true
print(df)
```
Slicing refers to rows when used directly on a DataFrame:

```{python}
#| echo: true
# Selects rows with these index (labels)
print(df['Florida':'Illinois']  )
```

```{python}
#| echo: true
# Selects rows by position
print(df[1:3])                
```

## Masking
```{python}
#| echo: true
print(df)
```
Direct masking is row-wise:

```{python}
#| echo: true
# Selects rows where density > 100
print(df[df.density > 100])
```

## Hands-on - Index and Slicing

```{python}
#| echo: true
import pandas as pd
import numpy as np
# Create Series for COVID-19 metrics
cases = pd.Series({'United States': 98.2, 'India': 44.7, 'Brazil': 37.6,'France': 35.4, 'Germany': 30.2})
deaths = pd.Series({'United States': 1.1, 'India': 0.53, 'Brazil': 0.69,'France': 0.16, 'Germany': 0.17})
population = pd.Series({'United States': 331.9, 'India': 1393.4, 'Brazil': 214.3, 'France': 67.8, 'Germany': 83.2})
# Create DataFrame
covid_df = pd.DataFrame({'cases_millions': cases, 
                         'deaths_millions': deaths, 
                         'population_millions': population})
```

## Hands-on - Index and Slicing

```{python}
#| echo: true
print("COVID-19 Statistics by Country:")
covid_df
```

## Hands-on - Index and Slicing

-   Access the number of cases in Brazil
-   Add a new column 'case_fatality_rate' calculated as **deaths divided by cases (as a percentage)**
-   Use `loc` to select data for Brazil and France, showing only cases and deaths
-   Use `iloc` to select the first 3 countries and all columns
-   Select countries with **case fatality rate greater than 1.5%**

# Operations in Pandas

## Universal Functions: Index Preservation

Pandas inherits NumPy's ability to perform **quick element-wise operations** while adding two key features:

-   **Index and column label preservation** for unary operations
-   **Automatic index alignment** for binary operations


## Universal Functions: Index Preservation

The indices are preserved

```{python}
#| echo: true
rng = np.random.RandomState(0)
df = pd.DataFrame(rng.randint(0, 10, (3, 4)),
                 columns=['A', 'B', 'C', 'D'])
print(df)                
```

```{python}
#| echo: true
print(np.sin(df * np.pi / 4))
```

## Index Alignment in Operations

When operating on two Series, Pandas automatically **aligns indices**.

```{python}
#| echo: true
area = pd.Series({'Alaska': 1723337, 'Texas': 695662,'California': 423967}, name='area')
population = pd.Series({'California': 38332521, 'Texas': 26448193, 'New York': 19651127}, name='population')

print(population)
print(area)
```

## Index Alignment in Operations

Default behavior produces NaN for missing indices

```{python}
#| echo: true
population / area
```

Using `div()` method with `fill_value`

```{python}
#| echo: true
population.div(area, fill_value=1) ## just for demo
```

## DataFrame Alignment

For DataFrames, alignment happens on both rows and columns.

```{python}
#| echo: true
A = pd.DataFrame(np.random.randint(0, 20, (2, 2)),
                columns=list('AB'))
print(A)
```

```{python}
#| echo: true
B = pd.DataFrame(np.random.randint(0, 10, (3, 3)),
                columns=list('BAC'))
print(B)
```

## DataFrame Alignment

Addition with automatic alignment

```{python}
#| echo: true
print(A + B)
```

Fill values can be specified for missing entries.
```{python}
#| echo: true
print(A.add(B, fill_value=0))
```

## Pandas Method Equivalents

| Python Operator | Pandas Method(s)                 |
|-----------------|----------------------------------|
| `+`             | `add()`                          |
| `-`             | `sub()`, `subtract()`            |
| `*`             | `mul()`, `multiply()`            |
| `/`             | `truediv()`, `div()`, `divide()` |
| `//`            | `floordiv()`                     |
| `%`             | `mod()`                          |
| `**`            | `pow()`                          |


## Hands-on - UFunc

```{python}
#| echo: true
# Create a DataFrame with COVID-19 data for the last 5 days
dates = pd.date_range(end='2023-04-30', periods=5)
data = {
    'United States': [45000, 42000, 38000, 41000, 39000],
    'India': [12000, 13500, 14200, 13800, 12500],
    'Brazil': [9800, 9200, 8700, 9100, 9500],
    'France': [7500, 7200, 6800, 6500, 6200],
    'Germany': [5200, 4800, 4600, 4900, 5100]
}

daily_cases = pd.DataFrame(data, index=dates)
daily_cases.index.name = 'Date'
daily_cases.columns.name = 'Country'
print("Daily New COVID-19 Cases:")
print(daily_cases)
```

## Hands-on - UFunc

-   Create a DataFrame showing the **percentage change** for all the days from the first day for each country
-   Subtract the **mean daily cases** for each country from the daily cases
-   Calculate the **ratio of daily cases** in the United States compared to each other country

# Handling Missing Data in Pandas

## Missing Data Representations (1/2)

Pandas uses two main values to represent missing data:

1.  **None**: A Python singleton object
    -   Operations on arrays with `None` values are performed at the Python level (**slower**)
    -   Aggregations like `sum()` or `min()` will generally result in errors
    
## Missing Data Representations (2/2)

Pandas uses two main values to represent missing data:

2.  **NaN (Not a Number)**: A special floating-point value
    -   Part of the IEEE floating-point specification
    -   Arrays with NaN maintain native data types (**faster operations**)
    -   NaN "infects" other values in arithmetic operations
    -   Aggregations return **NaN** unless using special functions like `np.nansum()`

## Detecting Null Values

```{python}
#| echo: true
import pandas as pd
import numpy as np
# Create Series for COVID-19 metrics
cases = pd.Series({'2020-01-01': 98.2, '2020-01-02': 99, '2020-01-03': np.nan,'2020-01-04': 150, '2020-01-05': 180})
deaths = pd.Series({'2020-01-01': np.nan, '2020-01-02': np.nan, '2020-01-03': np.nan,'2020-01-04': np.nan, '2020-01-05': np.nan})
population = pd.Series({'2020-01-01': 331.9, '2020-01-02':331.9, '2020-01-03': 331.9, '2020-01-04': 331.9, '2020-01-05': 331.9})
# Create DataFrame
covid_df = pd.DataFrame({'cases_millions': cases, 
                         'deaths_millions': deaths, 
                         'population_millions': population})
print("COVID-19 Statistics by Country:")
print(covid_df)
```
## Detecting Null Values
`df.isnull()`: Generate boolean mask of null values
```{python}
#| echo: true
print(covid_df.isnull())
```
`df.notnull()`: Opposite of isnull()
```{python}
#| echo: true
print(covid_df.notnull() )
```


## Dropping Null Values
```{python}
#| echo: true
print(covid_df)
```

`df.dropna()` default: drop `row`s

```{python}
#| echo: true
# Drop rows with any null values
print(covid_df.dropna())              
```
## Dropping Null Values
```{python}
#| echo: true
print(covid_df)
```
`axis='columns'`: Drop columns with any null values
```{python}
#| echo: true
print(covid_df.dropna(axis='columns'))  
```

## Dropping Null Values
```{python}
#| echo: true
print(covid_df)
```
`how='all'`: Drop **rows** with all null values
```{python}
#| echo: true
print(covid_df.dropna(how='all'))          
```
## Dropping Null Values
```{python}
#| echo: true
print(covid_df)
```
`axis=1, how='all'`: Drop **columns** with all null values
```{python}
#| echo: true
print(covid_df.dropna(axis=1, how='all'))
```
## Dropping Null Values
```{python}
#| echo: true
print(covid_df)
```
`thresh=3`: Keep rows with at least `2` non-null values
```{python}
#| echo: true
print(covid_df.dropna(thresh=2))          
```

## Filling Null Values
```{python}
#| echo: true
print(covid_df)
```
`df.fillna(value)`:Fill with a constant value
```{python}
#| echo: true
print(covid_df.fillna(0))
```
## Filling Null Values
```{python}
#| echo: true
print(covid_df)
```
`df.ffill()`: Forward-fill (propagate last valid value)
```{python}
#| echo: true
print(covid_df.ffill())
```

## Filling Null Values
```{python}
#| echo: true
print(covid_df)
```
`df.bfill()`: Back-fill (use next valid value)
```{python}
#| echo: true
print(covid_df.bfill())
```

## Hands-on - Missing Value

```{python}
#| echo: true
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Create a DataFrame with COVID-19 data (with some missing values)
data = {
    'Country': ['USA', 'India', 'Brazil', 'France', 'Germany', 'UK', 'Italy', 'Spain', 'Russia', 'Mexico'],
    'Cases': [98200000, 44700000, 37600000, 35400000, 30200000, np.nan, 25000000, 13800000, np.nan, 7300000],
    'Deaths': [1100000, 530000, 690000, 160000, 170000, 220000, np.nan, 120000, 400000, np.nan],
    'Recovered': [93100000, 43200000, 35900000, 34800000, 29500000, 24100000, None, 13600000, 18200000, 6900000],
    'Tests': [np.nan, 900000000, 180000000, 271000000, 122000000, 522000000, 265000000, 471000000, np.nan, 18000000]
}

covid_df = pd.DataFrame(data)
print("COVID-19 Dataset:")
print(covid_df)
```

## Hands-on - Missing Value

-   Check for missing values in the dataset using isnull() and calculate the sum for each column
-   Drop only rows where the 'Deaths' column has missing values (`Hint`: `subset` parameter in `df.dropna()`)
-   Use forward fill (ffill) to propagate the last valid observation forward
-   Create a new column 'Case_Fatality_Rate' calculated as Deaths/Cases, handling any resulting NaN values appropriately

# Hierarchical Indexing in Pandas

## Introduction to Multi-Indexing/Hierarchical indexing

-   Represent higher-dimensional data within the familiar 1D **Series** and 2D **DataFrame** objects.
-   Providing a powerful way to work with complex data structures.

## Creating a Multi-Indexed Series

Create a multi-indexed Series by using **tuples** as keys.

```{python}
#| echo: true
index = [('New York', 2000), ('New York', 2010),
          ('California', 2000), ('California', 2010),
         ('Texas', 2000), ('Texas', 2010)]
populations = [33871648, 37253956, 18976457, 19378102, 20851820, 25145561]
pop = pd.Series(populations, index=index)
print(pop)
```

## The Better Way: MultiIndex

A more efficient approach is to use Pandas' MultiIndex.

```{python}
#| echo: true
# Create MultiIndex from tuples
index = pd.MultiIndex.from_tuples(index)
print(index)
```

```{python}
#| echo: true
pop = pop.reindex(index)
print(pop)
```

## The Better Way: MultiIndex

We can easily select data using intuitive slicing.

```{python}
#| echo: true
# Select all data from 2010
pop[:, 2010]
```

## MultiIndex Construction Methods (1/2)

There are several ways to create a MultiIndex.

From arrays
```{python}
#| echo: true
pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'], [1, 2, 1, 2]])
```
From tuples
```{python}
#| echo: true
pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])
```

## MultiIndex Construction Methods (2/2)

From Cartesian product

```{python}
#| echo: true
pd.MultiIndex.from_product([['a', 'b'], [1, 2]])
```


## Naming Levels

You can name the levels for better clarity.

```{python}
#| echo: true
pop.index.names = ['state', 'year']
print(pop)
```

## MultiIndex for Columns

Original DataFrame:

```{python}
#| echo: true
HR_Bob = np.array([130,120,155,168])
HR_Guido = np.array([100,90,60,80])
HR_Sue = np.array([60,70,80,70])
Temp_Bob = np.array([36.7,38.2,37.6,36.2])
Temp_Guido = np.array([37.5,38,36.8,37.1])
Temp_Sue = np.array([37.7,38.2,36.9,35.7])
health_data = pd.DataFrame(np.column_stack((HR_Bob,Temp_Bob,HR_Guido,Temp_Guido,HR_Sue,Temp_Sue)))
print(health_data)
```

## MultiIndex for Columns

DataFrames can have multi-indexed columns as well:
```{python}
#| echo: true
# Create hierarchical indices and columns
index = pd.MultiIndex.from_product([[2013, 2014], [1, 2]], names=['year', 'visit'])
columns = pd.MultiIndex.from_product([['Bob', 'Guido', 'Sue'], ['HR', 'Temp']], names=['subject', 'type'])

health_data = pd.DataFrame(np.column_stack((HR_Bob,Temp_Bob,HR_Guido,Temp_Guido,HR_Sue,Temp_Sue)), 
              index=index, columns=columns)
print(health_data)
```

## Indexing and Slicing - Series
```{python}
#| echo: true
print(pop)
```
Access single elements
```{python}
#| echo: true
pop['California', 2000]
```
## Indexing and Slicing  - Series
```{python}
#| echo: true
print(pop)
```
Partial indexing (returns a Series)
```{python}
#| echo: true
pop['California']
```
## Indexing and Slicing - Series
```{python}
#| echo: true
print(pop)
```
Selecting specific level
```{python}
#| echo: true
pop[:, 2000]
```
## Indexing and Slicing - Series
```{python}
#| echo: true
print(pop)
```
Boolean masking
```{python}
#| echo: true
pop[pop > 22000000]
```
## Indexing and Slicing - Series
select multiple index by `[[]]`
```{python}
#| echo: true
print(pop)
```
```{python}
#| echo: true
pop[['California', 'Texas']]
```

## Indexing and Slicing - multi-indexed columns
```{python}
#| echo: true
print(health_data)
```
Select a specific person's data
```{python}
#| echo: true
print(health_data['Guido'])
```

## Indexing and Slicing - multi-indexed columns
```{python}
#| echo: true
print(health_data)
```
Select specific measurements
```{python}
#| echo: true
print(health_data['Guido', 'HR'])
```

## Sorting Indices
```{python}
#| echo: true
print(pop)
```
Sort index for proper slicing
```{python}
#| echo: true
pop_sort = pop.sort_index()
print(pop_sort)
```

## Stacking and Unstacking
```{python}
#| echo: true
print(pop)
```
Convert Series to DataFrame
```{python}
#| echo: true
pop_df = pop.unstack()
print("unstack: ")
print(pop_df)
```

## Index Manipulation
```{python}
#| echo: true
print(pop)
```
Convert index to columns
```{python}
#| echo: true
pop_flat = pop.reset_index(name='population')
print(pop_flat)
```

## Index Manipulation
```{python}
#| echo: true
print(pop)
```
Convert columns to index
```{python}
#| echo: true
print(pop_flat.set_index(['state', 'year']))
```

## Data Aggregations

```{python}
#| echo: true
print(health_data)
```

```{python}
#| echo: true
data_mean = health_data.mean()
print(data_mean)
```

## Data Aggregations

```{python}
#| echo: true
print(health_data)
```
Average by year
```{python}
#| echo: true
data_mean = health_data.groupby('year').mean()
print(data_mean)
```
## Data Aggregations
```{python}
#| echo: true
print(health_data)
```
Average by measurement type across columns
```{python}
#| echo: true
data_mean.T.mean() # just for demo
```

## Data Aggregations

```{python}
#| echo: true
print(health_data)
```
Average by measurement type across columns
```{python}
#| echo: true
print(data_mean.T.groupby('type').mean())
```

## Hands-on - Hierarchical Indexing

```{python}
#| echo: true
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# Create a healthcare dataset with patient information across different hospitals and departments
# Row index: Hospital, Department, Patient ID
# Column index: Metric Type, Measurement

# Create sample data
hospitals = ['General Hospital', 'General Hospital', 'Community Clinic', 'Community Clinic']
departments = ['Cardiology', 'Neurology'] * 2
patient_ids = list(range(1001, 1009))  # 8 patients

# Create row multi-index
row_tuples = [(h, d, p) for h, d in zip(hospitals * 2, departments * 2) for p in [patient_ids.pop(0)]]
row_index = pd.MultiIndex.from_tuples(row_tuples, names=['Hospital', 'Department', 'PatientID'])

# Create column multi-index
metric_types = ['Vital Signs', 'Vital Signs', 'Lab Results', 'Lab Results']
measurements = ['Blood Pressure', 'Heart Rate', 'Glucose', 'Cholesterol']
col_index = pd.MultiIndex.from_arrays([metric_types, measurements], names=['Type', 'Measurement'])

# Generate sample data
np.random.seed(42)
data = np.random.randint(60, 180, size=(8, 4))
health_df = pd.DataFrame(data, index=row_index, columns=col_index)
print("Healthcare Patient Data:")
print(health_df)
```

## Hands-on - Hierarchical Indexing

-   Select all vital signs for patients in the Cardiology department
-   Calculate the average values for each measurement by hospital
-   Create a boolean mask to identify patients with heart rate \> 100 and glucose \> 120
-   Reset the index to convert the hierarchical index to columns

# Combining Datasets: Concat and Append

## Helper Function

```{python}
#| echo: true
def make_df(cols, ind):
    """Quickly make a DataFrame"""
    data = {c: [str(c) + str(i) for i in ind]
            for c in cols}
    return pd.DataFrame(data, ind)

class display(object):
    """Display HTML representation of multiple objects"""
    template = """<div style="float: left; padding: 10px;">
    <p style='font-family:"Courier New", Courier, monospace'>{0}</p>{1}
    </div>"""
    def __init__(self, *args):
        self.args = args
        
    def _repr_html_(self):
        return '\n'.join(self.template.format(a, eval(a)._repr_html_())
                         for a in self.args)
    
    def __repr__(self):
        return '\n\n'.join(a + '\n' + repr(eval(a))
                           for a in self.args)
```


## Basic Concatenation - DataFrames

`pd.concat` Default: Concatenate vertically (`axis=0`)

```{python}
#| echo: true
df1 = make_df('AB', [1, 2]) 
df2 = make_df('AB', [3, 4])  
df12 = pd.concat([df1, df2])
display('df1', 'df2', 'df12')
```

## Concatenation - Axis Selection

You can concatenate column-wise by `axis=1`

```{python}
#| echo: true
df3 = make_df('AB', [0, 1])
df4 = make_df('CD', [0, 1])
df5 = pd.concat([df3, df4], axis=1) 
display('df3', 'df4', 'df5')
```

## Concatenation - Duplicate Indices

Pandas preserves indices during concatenation: can result in duplicate indices

```{python}
#| echo: true
x = make_df('AB', [0, 1])
y = make_df('AB', [0, 1])
display('x', 'y', 'pd.concat([x, y])')
```

## Handling Duplicate Indices

1.  **Verify integrity**: Raise an error if duplicates exist `verify_integrity=True`

```{python}
#| echo: true
try:
    pd.concat([x, y], verify_integrity=True)
except ValueError as e:
    print("ValueError:", e)
```
## Handling Duplicate Indices
2.  **Ignore indices**: Create new integer indices

```{python}
#| echo: true
index2 = pd.concat([x, y], ignore_index=True)
display('x', 'y', "index2")
```
## Handling Duplicate Indices
3.  **Add hierarchical keys**: Create a MultiIndex to distinguish sources

```{python}
#| echo: true
index3 = pd.concat([x, y], keys=['x', 'y'])
display('x', 'y', "index3")
```

## Concatenation 

When concatenating DataFrames with shared columns

```{python}
#| echo: true
df5 = make_df('AB', [1, 2])
df6 = make_df('BC', [2, 3])
display('df5', 'df6', 'pd.concat([df5, df6])')
```

## Hands-on - Concat and Append

Here is sales data from different regional offices that need to be combined for company-wide analysis.

```{python}
#| echo: true
import pandas as pd
import numpy as np

# Create sales data for different regions
def create_sales_data(region, periods, seed=None):
    if seed is not None:
        np.random.seed(seed)
    
    dates = pd.date_range(start='2025-01-01', periods=periods)
    products = ['Laptop', 'Phone', 'Tablet', 'Monitor']
    
    data = {
        'Date': np.random.choice(dates, size=periods),
        'Product': np.random.choice(products, size=periods),
        'Units': np.random.randint(1, 50, size=periods),
        'Price': np.random.uniform(100, 1500, size=periods).round(2),
        'Region': region
    }
    
    df = pd.DataFrame(data)
    df['Revenue'] = df['Units'] * df['Price']
    return df

# Create regional datasets
north_sales = create_sales_data('North', 20, seed=42)
south_sales = create_sales_data('South', 15, seed=43)
east_sales = create_sales_data('East', 25, seed=44)
west_sales = create_sales_data('West', 18, seed=45)
```
## Hands-on - Concat and Append
```{python}
#| echo: true
# Display the first few rows of each dataset
print("North Region Sales:")
print(north_sales.head(3))
print("\nSouth Region Sales:")
print(south_sales.head(3))
```

## Hands-on - Concat and Append

-   Combine all regional sales data into a single DataFrame
-   Reset indices as 'Date' in original DataFrames, then perfrom default concatenation (preserves original indices)
-   When perform concatenation, adding hierarchical keys to track source

# Combining Datasets: Merge and Join


## Relational Algebra in Pandas

Pandas provides high-performance, in-memory join and merge operations through the `pd.merge` function

1.  **One-to-one joins**: Combine datasets with unique keys in both DataFrames
2.  **Many-to-one joins**: Join when one DataFrame has duplicate keys
3.  **Many-to-many joins**: Join when both DataFrames have duplicate keys -> **should be avoided**

## Basic Join Example

Using the common column as the key.

```{python}
#| echo: true
df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})
df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],'hire_date': [2004, 2008, 2012, 2014]})
dfmerge = pd.merge(df1, df2)
```
## Basic Join Example
```{python}
display('df1','df2')
```
## Basic Join Example
```{python}
display('dfmerge')
```
## Specifying Merge Keys

Using the `on` Parameter

```{python}
#| echo: true
pd.merge(df1, df2, on='employee')
```

## Different Column Names

`left_on` and `right_on` Parameter

```{python}
#| echo: true
df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
                    'salary': [70000, 80000, 120000, 90000]})
print(df1)
print(df3)
print(pd.merge(df1, df3, left_on="employee", right_on="name"))
```

## Merging on Index

```{python}
#| echo: true
df1a = df1.set_index('employee')
df2a = df2.set_index('employee')
```

The DataFrame `.join()` method provides a shortcut for index-based merges:

```{python}
#| echo: true
print(df1a.join(df2a))
```

## Join Types: Inner join

`how` Parameter: **Inner join** (default): Returns only matching rows

```{python}
#| echo: true
df7 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],
                    'food': ['fish', 'beans', 'bread']},
                   columns=['name', 'food'])
df8 = pd.DataFrame({'name': ['Mary', 'Joseph'],
                    'drink': ['wine', 'beer']},
                   columns=['name', 'drink'])
innerpd = pd.merge(df7, df8, how='inner')
```
::::: columns
::: column
```{python}
print('df7')
print(df7)
```
:::
::: column
```{python}
print('df8')
print(df8)
```
:::
:::::

```{python}
print('innerpd')
print(innerpd)
```

## Join Types: Outer join

`how` Parameter: **Outer join**: Returns all rows from both DataFrames, filling missing values with NaN

```{python}
#| echo: true
outerpd = pd.merge(df7, df8, how='outer')
```

::::: columns
::: column
```{python}
print('df7')
print(df7)
```
:::
::: column
```{python}
print('df8')
print(df8)
```
:::
:::::

```{python}
print('outerpd')
print(outerpd)
```

## Join Types: Left join

`how` Parameter:  **Left join**: Returns all rows from the left DataFrame

```{python}
#| echo: true
leftpd = pd.merge(df7, df8, how='left')
```

::::: columns
::: column
```{python}
print('df7')
print(df7)
```
:::
::: column
```{python}
print('df8')
print(df8)
```
:::
:::::

```{python}
print('leftpd')
print(leftpd)
```
## Join Types: Right join

`how` Parameter:  **Right join**: Returns all rows from the right DataFrame

```{python}
#| echo: true
rightpd = pd.merge(df7, df8, how='right')
```

::::: columns
::: column
```{python}
print('df7')
print(df7)
```
:::
::: column
```{python}
print('df8')
print(df8)
```
:::
:::::

```{python}
print('rightpd')
print(rightpd)
```
## Handling Duplicate Column Names

When DataFrames have conflicting column names, Pandas adds suffixes:

```{python}
#| echo: true
# Default suffixes (_x and _y)
df9 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
                    'rank': [1, 2, 3, 4]})
df10 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
                    'rank': [3, 1, 4, 2]})
pd.merge(df9, df10, on="name")
```

## Handling Duplicate Column Names
Custom suffixes with `suffixes` parameter
```{python}
#| echo: true
# Custom suffixes
pd.merge(df9, df10, on="name", suffixes=["_L", "_R"])
```

## Hands-on: US States Data

```{python}
#| echo: true
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
pop_url = "https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-population.csv"
area_url = "https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-areas.csv"
abb_url = "https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-abbrevs.csv"
pop = pd.read_csv(pop_url)
areas = pd.read_csv(area_url)
abbrevs = pd.read_csv(abb_url)
```

## Hands-on: US States Data

::::: columns
::: column
```{python}
print('pop.head()')
print(pop.head())
```
:::
::: column
```{python}
print('areas.head()')
print(areas.head())
```
:::
::: column
```{python}
print('abbrevs.head()')
print(abbrevs.head())
```
:::
:::::

## Hands-on: US States Data

Merge multiple datasets to calculate population density:

1.  Merge population data `pop` with state abbreviations `abbrevs`
2.  Handle missing values and mismatches (?)
3.  Merge with area data `areas`
4.  Calculate and rank US states and territories by their 2010 population density

## Hint Q2 PR

```{python}
#| echo: true
print(pop.query("`state/region`	 == 'PR'").head())
```

Do we have PR in `abbrevs` table?

```{python}
#| echo: true
abbrevs.query("abbreviation == 'PR'")
```

## Hint for Q4 `.query()`

```{python}
#| echo: true
pop2010 = pop.query("year == 2010 & ages == 'total'")
pop2010.head()
```

## Hint for Q4 Sort

`df.sort_values(by='column name', ascending=False)`
```{python}
#| echo: true
pop2010.sort_values(by='population',ascending=False).head()
```

# Aggregation and Grouping in Pandas


## Basic Aggregation

For DataFrames, aggregations operate on each column (collapse rows)

```{python}
#| echo: true
df = pd.DataFrame({'A': [0.16, 0.06, 0.87, 0.60, 0.71],
                   'B': [0.02, 0.97, 0.83, 0.21, 0.18]})
print(df)
df.mean()
```

Use axis parameter to aggregate across rows instead (collapse columns)

```{python}
#| echo: true
df.mean(axis='columns')
```

## Basic Aggregation `describe()`

quick statistical summary of your data: `describe()`

```{python}
#| echo: true
planets = sns.load_dataset('planets')
print(planets.dropna().describe())
```

## GroupBy: Split, Apply, Combine

`.groupby()` for more sophisticated data analysis by:

1.  **Splitting** data into groups based on some criteria

2.  **Applying** a function to each group independently

3.  **Combining** the results into a data structure

## GroupBy: Split, Apply, Combine
![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/03.08-split-apply-combine.png)
[Source](https://jakevdp.github.io/PythonDataScienceHandbook/)

## GroupBy: Split, Apply, Combine

```{python}
#| echo: true
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],
                   'data': range(6)})
print(df)
print(df.groupby('key').sum())
```


## Working with GroupBy Objects

**Dispatch Methods**: Apply DataFrame methods to each group

```{python}
#| echo: true
df = planets.groupby('method')['year'].describe()
print(df)
```

## Advanced GroupBy Operations

```{python}
#| echo: true
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],
                   'data1': range(6),
                   'data2': rng.randint(0, 10, 6)},
                   columns = ['key', 'data1', 'data2'])
print(df)
```

## GroupBy Operations - Aggregation

**Aggregation**: Multiple aggregations at once. 

```{python}                                                                                   
#| echo: true
df.groupby('key').aggregate(['min', 'median', 'max'])
```
## GroupBy Operations - Aggregation
Different aggregations per column

```{python}
#| echo: true
df.groupby('key').aggregate({'data1': 'min', 'data2': 'max'})
```

## GroupBy Operations - Filtering

**Filtering**: Keep only groups that satisfy a condition

```{python}
#| echo: true
def filter_func(x):
    return x['data2'].std() > 4
agg_df = df.groupby('key').filter(filter_func)
```
::::: columns
::: column
```{python}
print('df')
print(print(df))
```
:::
::: column
```{python}
print('agg_df')
print(print(agg_df))
```
:::
:::::
## GroupBy Operations - Transformation

**Transformation**: Apply a transformation that preserves the input shape

```{python}
#| echo: true
trans_df = df.groupby('key').transform(lambda x: x - x.mean())
```
::::: columns
::: column
```{python}
print('df')
print(print(df))
```
:::
::: column
```{python}
print('trans_df')
print(print(trans_df))
```
:::
:::::
## GroupBy Operations - Apply

**Apply**: Apply arbitrary functions to each group

```{python}
#| echo: true
def norm_by_data2(x):
    # x is a DataFrame of group values
    x['data1'] /= x['data2'].sum()
    return x
app_df = df.groupby('key').apply(norm_by_data2)
```
::::: columns
::: column
```{python}
print('df')
print(print(df))
```
:::
::: column
```{python}
print('app_df')
print(print(app_df))
```
:::
:::::

## Practical Example - Groupby

Analyzing planet discovery methods by decade

```{python}
#| echo: true
decade = 10 * (planets['year'] // 10)
decade = decade.astype(str) + 's'
planets['decade'] = decade
print(planets)
```
## Practical Example - Groupby

```{python}
#| echo: true
planets.groupby(['method', 'decade'])['number'].sum()
```

## Practical Example - Groupby
`.unstack()`
```{python}
#| echo: true
unstack_df = planets.groupby(['method', 'decade'])['number'].sum().unstack().fillna(0)
print(unstack_df)
```

## Pivot Tables in Pandas

Pivot tables are a powerful data analysis tool that provide multidimensional summarization of data. They can be thought of as a **multidimensional version** of GroupBy aggregation.

-   Transforms column-wise data into a two-dimensional table
-   Allows for multiple levels of indices and columns
-   Supports various aggregation functions

## Example: Titanic Dataset

```{python}
#| echo: true
import pandas as pd
import seaborn as sns
titanic = sns.load_dataset('titanic')
print(titanic)
```
## Titanic Dataset - Basic pivot table
Default: `aggfunc='mean'`
```{python}
#| echo: true
survival_by_class = titanic.pivot_table('survived', index='sex', columns='class')
print(survival_by_class)
```

## Titanic Dataset - Multi-level 
```{python}
#| echo: true
age_groups = pd.cut(titanic['age'], [0, 18, 80])
survival_by_age_class = titanic.pivot_table('survived', ['sex', age_groups], 'class')
print(survival_by_age_class)
```

## Additional Options

-   `aggfunc`: Specify aggregation function (e.g., 'sum', 'mean', 'count')
-   `fill_value`: Replace missing values
-   `margins`: Add row/column totals
-   `dropna`: Remove rows with missing values

## Practical Application: US Birth Data
```{python}
#| echo: true
births = pd.read_csv('https://raw.githubusercontent.com/jakevdp/data-CDCbirths/master/births.csv')
births['decade'] = 10 * (births['year'] // 10)
print(births)
```

## Practical Application: US Birth Data
Analyze births by decade and gender
```{python}
#| echo: true
births_by_decade = births.pivot_table('births', index='decade', 
                              columns='gender', aggfunc='sum')
print(births_by_decade)
```
## Practical Application: US Birth Data
Visualize trends
```{python}
#| echo: true
births_by_decade.plot()
```


## Hands-on: Aggregation
```{python}
#| echo: true
import pandas as pd
# Download the data
births = pd.read_csv('https://raw.githubusercontent.com/jakevdp/data-CDCbirths/master/births.csv')
births['decade'] = 10 * (births['year'] // 10)
print(births.head())
```
## Hands-on: Aggregation

-   Group the data by gender and calculate the total number of births for each gender.
-   Group by both year and gender, then sum the births.
-   Filter out years where the total number of births (all genders combined) was less than 3 million.
-   Create a pivot table with decade as rows, gender as columns, and the sum of births as values.
-   Create a pivot table showing the average number of births per month for each gender.

# Working with Time Series in Pandas

## Types of Time Data

Pandas provides extensive tools for working with time-based data, reflecting its origins in financial modeling:

-   **Time stamps**: Reference specific moments (e.g., July 4th, 2015 at 7:00am)
-   **Time intervals/periods**: Reference lengths of time between points (e.g., the year 2015)
-   **Time deltas/durations**: Reference exact time lengths (e.g., 22.56 seconds)


## Native Python (datetime)

```{python}
#| echo: true
from datetime import datetime
datetime(year=2015, month=7, day=4)
```

## NumPy's datetime64

```{python}
#| echo: true
import numpy as np
date = np.array('2015-07-04', dtype=np.datetime64)
date + np.arange(12)  # Array of consecutive dates
```

## Pandas Time Objects

Pandas combines the ease-of-use of datetime with the efficiency of numpy.datetime64:

```{python}
#| echo: true
import pandas as pd
date = pd.to_datetime("4th of July, 2015")
date.strftime('%A')  # 'Saturday'
```

## Parameters for date.strftime 
[Ref](https://strftime.org/)

| Code    | Description                                                                 | Example           |
|---------|-----------------------------------------------------------------------------|--------------------------|
| `%a`    | Abbreviated weekday name                                                    | Sun, Mon, …              |
| `%A`    | Full weekday name                                                           | Sunday, Monday, …        |
| `%w`    | Weekday as a decimal number (0=Sunday, 6=Saturday)                          | 0, 1, …, 6               |
| `%d`    | Day of the month (zero-padded)                                              | 01, 02, …, 31            |
| `%b`    | Abbreviated month name                                                      | Jan, Feb, …, Dec         |
| `%B`    | Full month name                                                             | January, February, …     |
| `%m`    | Month as a zero-padded decimal number                                       | 01, 02, …, 12            |

## Pandas Time Series Data Structures

-   **Timestamp**: Replacement for Python's datetime
-   **DatetimeIndex**: Index structure for timestamps
-   **Period/PeriodIndex**: For fixed-frequency time intervals
-   **Timedelta/TimedeltaIndex**: For time durations

## Creating Date Ranges
Daily frequency (default)
```{python}
#| echo: true
pd.date_range('2015-07-03', '2015-07-10')
```
Hourly frequency
```{python}
#| echo: true
pd.date_range('2015-07-03', periods=8, freq='h')
```
## Creating Date Ranges
Monthly periods
```{python}
#| echo: true
pd.period_range('2015-07', periods=8, freq='M')
```

## Time Series Indexing

```{python}
#| echo: true
index = pd.DatetimeIndex(['2014-07-04', '2014-08-04', 
                          '2015-07-04', '2015-08-04'])
data = pd.Series([0, 1, 2, 3], index=index)
print(data)
```
## Time Series Indexing
Slice by date range
```{python}
#| echo: true
data['2014-07-04':'2015-07-04']
```
Slice by year
```{python}
#| echo: true
data['2015']
```


## Sample dataset
```{python}
#| echo: true
#!pip3 install yfinance
import yfinance as yf
dat = yf.Ticker("MSFT")
MSFT = dat.history(period='1y')
print(MSFT.head())
```
## Frequency and Offset Codes

Common frequency codes:

::::: columns
::: column

- `D`: Calendar day 
- `B`: Business day 
- `W`: Weekly 
- `ME`: Month end 

:::
::: column

- `QE`: Quarter end 
- `YE`: Year end 
- `h`: Hours 
- `min`: Minutes

:::
:::::

## Time Series Operations - Resampling
Mean for each year
```{python}
#| echo: true
year_m = MSFT.resample('YE').mean()  
print(year_m)
```
## Time Series Operations - Resampling
Mean for each Quarter/bussiness day
```{python}
#| echo: true
qua_m = MSFT.resample('BQE').mean()  
print(qua_m)
```
## Time Series Operations - Resampling
Value at end of each year
```{python}
#| echo: true
year_v = MSFT.asfreq('YE')  
print(year_v)
year_c = MSFT.resample('YE').count()  
print(year_c)
```

## Time Series Operations - Time-shifting

```{python}
#| echo: true
# Shift data values
print(MSFT.head())
```
## Time Series Operations - Time-shifting
```{python}
#| echo: true
# Shift data values
print(MSFT.shift(3).head())
```

## Time Series Operations - Rolling Windows
monthly Rolling mean
```{python}
#| echo: true
y_rolling = MSFT.rolling(30)
y_r_mean = y_rolling.mean().tail()
print(y_r_mean)
```
## Time Series Operations - Rolling Windows
weekly Rolling mean
```{python}
#| echo: true
w_rolling = MSFT.rolling(7)
w_r_mean = w_rolling.mean().tail()  # 
print(w_r_mean)
```



## Hands-on - Time and Pandas

```{python}
#| echo: true
# Create a DataFrame with COVID-19 data for the last 5 days
dates = pd.date_range(end='2023-04-30', periods=5)
data = {
    'United States': [45000, 42000, 38000, 41000, 39000],
    'India': [12000, 13500, 14200, 13800, 12500],
    'Brazil': [9800, 9200, 8700, 9100, 9500],
    'France': [7500, 7200, 6800, 6500, 6200],
    'Germany': [5200, 4800, 4600, 4900, 5100]
}

daily_cases = pd.DataFrame(data, index=dates)
daily_cases.index.name = 'Date'
daily_cases.columns.name = 'Country'
print("Daily New COVID-19 Cases:")
print(daily_cases)
```

## Hands-on - Time and Pandas
-   Select and print the case numbers for India between '2023-04-27' and '2023-04-29' (inclusive).
-   Calculate the daily change (difference) in cases for Brazil.
-   Resample the data to a weekly frequency and calculate the mean for each country. (Hint: `.resample('W').mean()`)
-   Showing the 3-day moving average of cases for each country
-   For each country, identify the date with the highest number of new cases.

