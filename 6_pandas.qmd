---
title: "6. Pandas"
author: "Yi-Ju Tseng"
format:
  revealjs:
    slide-number: c/t
    show-slide-number: all
editor: visual
---

## Pandas

-   Introduction to Pandas Objects
-   Data Indexing and Selection in Pandas
-   Operations in Pandas
-   Handling Missing Data in Pandas
-   Hierarchical Indexing in Pandas
-   Combining Datasets: Concat and Append
-   Combining Datasets: Merge and Join
-   Aggregation and Grouping in Pandas
-   Working with Time Series in Pandas

# Introduction to Pandas Objects

## Introduction to Pandas Objects

-   Pandas objects are enhanced versions of NumPy arrays
    -   we need more flexibility (e.g., attaching labels to data, working with missing data, etc.)
-   Rows and columns identified with **labels** instead of integer indices
-   Three fundamental data structures:
    -   Series
    -   DataFrame
    -   Index

## Pandas Getting Started

Install

```{python}
#| eval: false
#| echo: true
!pip3 install numpy pandas # ! for shell script
```

Import

```{python}
#| echo: true
import numpy as np
import pandas as pd
```

## The Pandas Series Object

-   One-dimensional array of indexed data
-   Wraps both values and indices
-   `pd.Series(np 1D array or list or dict)`
    -   From list (default integer index)
    -   With custom index
    -   From dictionary

```{python}
#| echo: true
data = pd.Series([0.25, 0.5, 0.75, 1.0])
```

## Series: Data Access

```{python}
#| echo: true
data
```

Position (interger-based index)
```{python}
#| echo: true
# Access by position
data[1] 
```
Slicing
```{python}
#| echo: true
# Slicing works too
data[1:3]    
```

## Series: Flexible Indices

-   Pandas Series has an **defined index** associated with the values
-   Can use strings or other types

`pd.Series(list or np 1D array,index=)`

```{python}
#| echo: true
data = pd.Series([0.25, 0.5, 0.75, 1.0],
                 index=['a', 'b', 'c', 'd'])
print(data)
data['b']
```

## Series as Specialized Dictionary

-   **keys**: index
-   **values**: values

```{python}
#| echo: true
population_dict = {'California': 38332521,
                   'Texas': 26448193,
                   'New York': 19651127,
                   'Florida': 19552860,
                   'Illinois': 12882135}
population = pd.Series(population_dict)
print(population)
```

## Series: Dictionary-like Operations

```{python}
#| echo: true
population
```

Index (Dictionary-style access)

```{python}
#| echo: true
population['California'] 
```

Slicing! (Array-style operations) **Includes the end point**

```{python}
#| echo: true
population['California':'New York']
```

## Hands-on - Series

-   Create a Series containing the following student scores (out of 100): 85, 92, 78, 90, 88, and use student names as the index: 'Alice', 'Bob', 'Charlie', 'David', 'Emma'.
-   Series Operations
    -   Find Bob's score
    -   Find the average score
    -   Find all scores above 85 (`Hint`: Boolean)
    -   Add 5 points to everyone's score (`Hint`: +)

## The Pandas DataFrame Object

-   Two-dimensional data structure
    -   with both flexible **row indices** and flexible **column names**
-   DataFrame Construction Methods
    -   From a single Series
    -   From a two-dimensional NumPy array
    -   From a dictionary of Series

## DataFrame: From a Single Series

`pd.DataFrame(Series, columns=)`

```{python}
#| echo: true
pd.DataFrame(population, columns=['population'])
```

## DataFrame: From 2D NumPy Array

```{python}
#| echo: true
np.random.rand(3, 2)
```

`pd.DataFrame(np 2D array, columns=,index=)`
```{python}
#| echo: true
pd.DataFrame(np.random.rand(3, 2),
             columns=['foo', 'bar'],
             index=['a', 'b', 'c'])
```

## DataFrame: From a Dictionary of Series

```{python}
#| echo: true
# Create area Series
area_dict = {'California': 423967, 'Texas': 695662, 
             'New York': 141297, 'Florida': 170312, 
             'Illinois': 149995}
area = pd.Series(area_dict)
print('area')
print(area)
print('population')
print(population)
```
## DataFrame: From a Dictionary of Series
`pd.DataFrame({key:value, key:value})`

-   `key` as column name
-   `value` can be Series or Dictionary

```{python}
#| echo: true
states = pd.DataFrame({'population': population,
                       'area': area})
print(states)
```

## DataFrame: Structure
```{python}
#| echo: true
print(states)
```


Access index (row labels)

```{python}
#| echo: true
print(states.index)
```

Access columns

```{python}
#| echo: true
print(states.columns)
```

## DataFrame as Dictionary of Series

```{python}
#| echo: true
print(states)
```

Access a column values (returns a Series) with `['column name']`

```{python}
#| echo: true
states['area']
```

## The Pandas Index Object

-   Used by both **Series** and **DataFrame**
-   Can be viewed as:
    -   Immutable array
    -   Ordered set

```{python}
#| echo: true
ind = pd.Index([2, 3, 5, 7, 11])
print(ind)
```

## Summary

-   **Series**: 1D array with labeled index
-   **DataFrame**: 2D table with labeled rows and columns
-   **Index**: Immutable array-like structure for labeling

## Hands-on - DataFrame

You have information about countries stored in dictionaries. Convert this to a DataFrame `countries_df`.

```{python}
#| echo: true     
# Midyear Population
population_dict = {
    'USA': 331002651,
    'China': 1439323776,
    'India': 1380004385,
    'Brazil': 212559417,
    'Japan': 126476461
}
# Area (square km)
area_dict = {
    'USA': 9833517,
    'China': 9596960,
    'India': 3287263,
    'Brazil': 8515767,
    'Japan': 377975
}
#GDP
gdp_dict = {
    'USA': 21433225,
    'China': 14342903,
    'India': 2875142,
    'Brazil': 1839758,
    'Japan': 5081770
}
```

## Hands-on - DataFrame

-   Add the following columns to the `countries_df` (`Hint`: like numpy 2D array):
    -   Population density (population per square km)
    -   GDP per capita (gross domestic product divided by midyear population)

## Pandas

-   Introduction to Pandas Objects
-   **Data Indexing and Selection in Pandas**
-   Operations in Pandas
-   Handling Missing Data in Pandas
-   Hierarchical Indexing in Pandas
-   Combining Datasets: Concat and Append
-   Combining Datasets: Merge and Join
-   Aggregation and Grouping in Pandas
-   Working with Time Series in Pandas

# Data Indexing and Selection in Pandas

## DataFrame Indexing

```{python}
#| echo: true
area = pd.Series({'California': 423967, 'Texas': 695662,
                  'New York': 141297, 'Florida': 170312,
                  'Illinois': 149995})
pop = pd.Series({'California': 38332521, 'Texas': 26448193,
                 'New York': 19651127, 'Florida': 19552860,
                 'Illinois': 12882135})
df = pd.DataFrame({'area':area, 'pop':pop})
print(df)
```

## DataFrame Access

```{python}
#| echo: true
print(df)
```

**Column access**

**Dictionary-style indexing**: Access values by index (labels) `df['column name']`
```{python}
#| echo: true
df['area']  
```
## DataFrame Access

```{python}
#| echo: true
print(df)
```

**Column access**

**Attribute-style access**: For string index (labels) `df.index`

```{python}
#| echo: true
df.area   
```

## DataFrame Adding New Columns

```{python}
#| echo: true
print(df)
```

**Adding new columns**: `df['new column name'] = any values`

```{python}
#| echo: true
df['density'] = df['pop'] / df['area']
print(df)
```

## Special Indexers - `.loc`

```{python}
#| echo: true
print(df)
```

**loc**: Label-based indexing 

`df.loc[`row selection `,` column selection`]`

-   slice `start:end`
-   label list `[lable1, label2]`
```{python}
#| echo: true
print(df.loc[:'Illinois',['area','pop']])
```
## Special Indexers - `.iloc`

```{python}
#| echo: true
print(df)
```
**iloc**: Integer-based indexing 

`df.iloc[`row selection `,` column selection`]`

-   slice `start:end`
-   position list `[position1, position2]`

```{python}
#| echo: true
print(df.iloc[:3, [1,2]])
```

## Additional Indexing Conventions

```{python}
#| echo: true
print(df)
```

Slicing refers to **rows** when used directly on a DataFrame:

Selects rows with these index (labels)

```{python}
#| echo: true
print(df['Florida':'Illinois']  )
```
Selects rows by position
```{python}
#| echo: true
print(df[1:3])                
```

## Masking

```{python}
#| echo: true
print(df)
```
```{python}
#| echo: true
df.density > 100
```

Direct masking is row-wise: Selects rows where density > 100

```{python}
#| echo: true
print(df[df.density > 100])
```

## Hands-on - Index and Slicing

```{python}
#| echo: true
import pandas as pd
import numpy as np
# Create Series for COVID-19 metrics
cases = pd.Series({'United States': 98.2, 'India': 44.7, 'Brazil': 37.6,'France': 35.4, 'Germany': 30.2})
deaths = pd.Series({'United States': 1.1, 'India': 0.53, 'Brazil': 0.69,'France': 0.16, 'Germany': 0.17})
population = pd.Series({'United States': 331.9, 'India': 1393.4, 'Brazil': 214.3, 'France': 67.8, 'Germany': 83.2})
# Create DataFrame
covid_df = pd.DataFrame({'cases_millions': cases, 
                         'deaths_millions': deaths, 
                         'population_millions': population})
```

## Hands-on - Index and Slicing

```{python}
#| echo: true
print("COVID-19 Statistics by Country:")
covid_df
```

## Hands-on - Index and Slicing

-   Access the number of cases in Brazil
-   Add a new column 'case_fatality_rate' calculated as **deaths divided by cases (as a percentage)**
-   Use `loc` to select data for Brazil and France, showing only cases and deaths
-   Use `iloc` to select the first 3 countries and all columns
-   Select countries with **case fatality rate greater than 1.5%**

## Summary - DataFrame Access

```{python}
#| echo: true
print(df)
```
## Summary - DataFrame Access
Column access by `df[index or index list]`
```{python}
#| echo: true
print(df['area'])
```
```{python}
#| echo: true
print(df[['area','pop']])
```

## Summary - DataFrame Access
**Row slice**: default is row slicing

`df[index slice or interger-position slice for row]`

```{python}
#| echo: true
print(df['California':'New York'])
```
## Summary - DataFrame Access
**Row slice** and **column selection**: use `.loc` or `.iloc`

`df.loc[index slice for row , for columns]`: label based
```{python}
#| echo: true
print(df.loc['California':'New York','area'])
```
`df.iloc[index slice for row , for columns]`: integer-position based
```{python}
#| echo: true
print(df.iloc[:2,[0,2]])
```

## Pandas

-   Introduction to Pandas Objects
-   Data Indexing and Selection in Pandas
-   **Operations in Pandas**
-   Handling Missing Data in Pandas
-   Hierarchical Indexing in Pandas
-   Combining Datasets: Concat and Append
-   Combining Datasets: Merge and Join
-   Aggregation and Grouping in Pandas
-   Working with Time Series in Pandas

# Operations in Pandas

## Universal Functions: Index Preservation

Pandas inherits NumPy's ability to perform **quick element-wise operations** while adding two key features:

-   **Index and column label preservation** for unary operations
-   **Automatic index alignment** for binary operations

## Universal Functions: Index Preservation

The indices are preserved

```{python}
#| echo: true
rng = np.random.RandomState(0)
df = pd.DataFrame(rng.randint(0, 10, (3, 4)),
                 columns=['A', 'B', 'C', 'D'])
print(df)                
```

```{python}
#| echo: true
print(np.sin(df * np.pi / 4))
```

## Index Alignment in Operations

When operating on two Series, Pandas automatically **aligns indices**.

```{python}
#| echo: true
area = pd.Series({'Alaska': 1723337, 'Texas': 695662,'California': 423967}, name='area')
population = pd.Series({'California': 38332521, 'Texas': 26448193, 'New York': 19651127}, name='population')

print(population)
print(area)
```

## Index Alignment in Operations

Default behavior produces NaN for missing indices

```{python}
#| echo: true
population / area
```

Using `div()` method with `fill_value`

```{python}
#| echo: true
population.div(area, fill_value=1) ## just for demo
```

## DataFrame Alignment

For DataFrames, alignment happens on both rows and columns.

```{python}
#| echo: true
df1 = pd.DataFrame(np.random.randint(0, 20, (2, 2)),
                columns=list('AB'))
print(df1)
```

```{python}
#| echo: true
df2 = pd.DataFrame(np.random.randint(0, 10, (3, 3)),
                columns=list('BAC'))
print(df2)
```

## DataFrame Alignment

Addition with automatic alignment

```{python}
#| echo: true
print(df1 + df2)
```

Fill values can be specified for missing entries.

```{python}
#| echo: true
print(df1.add(df2, fill_value=0))
```

## Pandas Method Equivalents

| Python Operator | Pandas Method(s)                 |
|-----------------|----------------------------------|
| `+`             | `add()`                          |
| `-`             | `sub()`, `subtract()`            |
| `*`             | `mul()`, `multiply()`            |
| `/`             | `truediv()`, `div()`, `divide()` |
| `//`            | `floordiv()`                     |
| `%`             | `mod()`                          |
| `**`            | `pow()`                          |

## Hands-on - UFunc

```{python}
#| echo: true
# Create a DataFrame with COVID-19 data for the last 5 days
dates = pd.date_range(end='2023-04-30', periods=5)
data = {
    'United States': [45000, 42000, 38000, 41000, 39000],
    'India': [12000, 13500, 14200, 13800, 12500],
    'Brazil': [9800, 9200, 8700, 9100, 9500],
    'France': [7500, 7200, 6800, 6500, 6200],
    'Germany': [5200, 4800, 4600, 4900, 5100]
}

daily_cases = pd.DataFrame(data, index=dates)
daily_cases.index.name = 'Date'
daily_cases.columns.name = 'Country'
print("Daily New COVID-19 Cases:")
print(daily_cases)
```

## Hands-on - UFunc

-   Create a DataFrame that displays the **percentage change** for each country on all days, comparing the values to the first row.
-   For each date, subtract the **mean daily cases** across all countries from the daily cases reported for that date.
-   Calculate the **ratio of daily cases** in the United States compared to each other country

## Pandas

-   Introduction to Pandas Objects
-   Data Indexing and Selection in Pandas
-   Operations in Pandas
-   **Handling Missing Data in Pandas**
-   Hierarchical Indexing in Pandas
-   Combining Datasets: Concat and Append
-   Combining Datasets: Merge and Join
-   Aggregation and Grouping in Pandas
-   Working with Time Series in Pandas

# Handling Missing Data in Pandas

## Missing Data Representations (1/2)

Pandas uses two main values to represent missing data:

1.  **None**: A Python singleton object
    -   Operations on arrays with `None` values are performed at the Python level (**slower**)
    -   Aggregations like `sum()` or `min()` will generally result in errors

## Missing Data Representations (2/2)

Pandas uses two main values to represent missing data:

2.  **NaN (Not a Number)**: A special floating-point value
    -   Part of the IEEE floating-point specification
    -   Arrays with NaN maintain native data types (**faster operations**)
    -   NaN "infects" other values in arithmetic operations
    -   Aggregations return **NaN** unless using special functions like `np.nansum()`

## Detecting Null Values

```{python}
#| echo: true
import pandas as pd
import numpy as np
# Create Series for COVID-19 metrics
cases = pd.Series({'2020-01-01': 98.2, '2020-01-02': 99, '2020-01-03': np.nan,'2020-01-04': 150, '2020-01-05': 180})
deaths = pd.Series({'2020-01-01': np.nan, '2020-01-02': np.nan, '2020-01-03': np.nan,'2020-01-04': np.nan, '2020-01-05': np.nan})
population = pd.Series({'2020-01-01': 331.9, '2020-01-02':331.9, '2020-01-03': 331.9, '2020-01-04': 331.9, '2020-01-05': 331.9})
# Create DataFrame
covid_df = pd.DataFrame({'cases_millions': cases, 
                         'deaths_millions': deaths, 
                         'population_millions': population})
print("COVID-19 Statistics by Country:")
print(covid_df)
```

## Detecting Null Values

`df.isnull()`: Generate boolean mask of null values

```{python}
#| echo: true
print(covid_df.isnull())
```

`df.notnull()`: Opposite of isnull()

```{python}
#| echo: true
print(covid_df.notnull() )
```

## Dropping Null Values

```{python}
#| echo: true
print(covid_df)
```

`df.dropna()` default: drop `row`s

```{python}
#| echo: true
# Drop rows with any null values
print(covid_df.dropna())              
```

## Dropping Null Values

```{python}
#| echo: true
print(covid_df)
```

`df.dropna()` with `axis='columns'`: Drop columns with any null values

```{python}
#| echo: true
print(covid_df.dropna(axis='columns'))  
```

## Dropping Null Values

```{python}
#| echo: true
print(covid_df)
```

`df.dropna()` with `how='all'`: Drop **rows** with **all** null values

```{python}
#| echo: true
print(covid_df.dropna(how='all'))          
```

## Dropping Null Values

```{python}
#| echo: true
print(covid_df)
```

`df.dropna()` with `axis=1, how='all'`: Drop **columns** with **all** null values

```{python}
#| echo: true
print(covid_df.dropna(axis=1, how='all'))
```

## Dropping Null Values

```{python}
#| echo: true
print(covid_df)
```

`thresh=3`: Drop rows with `>=3` null values

```{python}
#| echo: true
print(covid_df.dropna(thresh=2))          
```

## Filling Null Values

```{python}
#| echo: true
print(covid_df)
```

`df.fillna(value)`:Fill with a constant value

```{python}
#| echo: true
print(covid_df.fillna(0))
```

## Filling Null Values

```{python}
#| echo: true
print(covid_df)
```

`df.ffill()`: Forward-fill (propagate last valid value)

```{python}
#| echo: true
print(covid_df.ffill()) ## reasonable in healthcare settings
```

## Filling Null Values

```{python}
#| echo: true
print(covid_df)
```

`df.bfill()`: Back-fill (use next valid value)

```{python}
#| echo: true
print(covid_df.bfill())
```

## Hands-on - Missing Value

```{python}
#| echo: true
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data = {
    'Country': ['USA', 'India', 'Brazil', 'France', 'Germany', 'UK', 'Italy', 'Spain', 'Russia', 'Mexico'],
    'Cases': [98200000, 44700000, 37600000, 35400000, 30200000, np.nan, 25000000, 13800000, np.nan, 7300000],
    'Deaths': [1100000, 530000, 690000, 160000, 170000, 220000, np.nan, 120000, 400000, np.nan],
    'Recovered': [93100000, 43200000, 35900000, 34800000, 29500000, 24100000, None, 13600000, 18200000, 6900000],
    'Tests': [np.nan, 900000000, 180000000, 271000000, 122000000, 522000000, 265000000, 471000000, np.nan, 18000000]
}

covid_df = pd.DataFrame(data)
print("COVID-19 Dataset:")
print(covid_df)
```

## Hands-on - Missing Value

-   Check for missing values in the dataset using `isnull()` and calculate the sum for each column (handling any NaN values appropriately)
-   Drop only rows where the 'Deaths' column has missing values (`Hint`: `subset` parameter in `df.dropna()`)
-   Use forward fill (`df.ffill()`) to propagate the last valid observation forward
-   Create a new column 'Case_Fatality_Rate' calculated as Deaths/Cases, handling any resulting `NaN` values appropriately

## Pandas

-   Introduction to Pandas Objects
-   Data Indexing and Selection in Pandas
-   Operations in Pandas
-   Handling Missing Data in Pandas
-   **Hierarchical Indexing in Pandas**
-   Combining Datasets: Concat and Append
-   Combining Datasets: Merge and Join
-   Aggregation and Grouping in Pandas
-   Working with Time Series in Pandas

# Hierarchical Indexing in Pandas

## Introduction to Multi-Indexing/Hierarchical indexing

-   Represent higher-dimensional data within the familiar 1D **Series** and 2D **DataFrame** objects.
-   Providing a powerful way to work with complex data structures.

## Creating a Multi-Indexed Series

Create a multi-indexed Series by using **tuples** as keys.

```{python}
#| echo: true
index = [('New York', 2000), ('New York', 2010),
          ('California', 2000), ('California', 2010),
         ('Texas', 2000), ('Texas', 2010)]
populations = [33871648, 37253956, 18976457, 19378102, 20851820, 25145561]
pop = pd.Series(populations, index=index)
print(pop)
```

## The Better Way: MultiIndex (1/2)

A more efficient approach is to use Pandas' MultiIndex. 

1. From tuples `pd.MultiIndex.from_tuples`

```{python}
#| echo: true
index = pd.MultiIndex.from_tuples([('New York', 2000), ('New York', 2010),
          ('California', 2000), ('California', 2010),
         ('Texas', 2000), ('Texas', 2010)])
print(index)
```


Use `.index = new index` to assign new index
```{python}
#| echo: true
pop.index = index
print(pop)
```

## MultiIndex Construction Methods (2/2)
2. From arrays `pd.MultiIndex.from_arrays()`

```{python}
#| echo: true
pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'], [1, 2, 1, 2]])
```

3. From Cartesian product `pd.MultiIndex.from_product()`

```{python}
#| echo: true
pd.MultiIndex.from_product([['a', 'b'], [1, 2]])
```

## Naming Levels

You can name the levels for better clarity.

```{python}
#| echo: true
pop.index.names = ['state', 'year']
print(pop)
```


## MultiIndex for Columns

Original DataFrame:

```{python}
#| echo: true
HR_Bob = np.array([130,120,155,168])
HR_Guido = np.array([100,90,60,80])
HR_Sue = np.array([60,70,80,70])
Temp_Bob = np.array([36.7,38.2,37.6,36.2])
Temp_Guido = np.array([37.5,38,36.8,37.1])
Temp_Sue = np.array([37.7,38.2,36.9,35.7])
health_data = pd.DataFrame(np.column_stack((HR_Bob,Temp_Bob,HR_Guido,Temp_Guido,HR_Sue,Temp_Sue)))
print(health_data)
```

## MultiIndex for Columns

DataFrames can have multi-indexed columns as well:
```{python}
#| echo: true
ind = pd.MultiIndex.from_product([[2013, 2014], [1, 2]], names=['year', 'visit'])
print(ind)
```
```{python}
#| echo: true
col = pd.MultiIndex.from_product([['Bob', 'Guido', 'Sue'], ['HR', 'Temp']], names=['subject', 'type'])
print(col)
```
## MultiIndex for Columns
```{python}
#| echo: true
health_data = pd.DataFrame(np.column_stack((HR_Bob,Temp_Bob,HR_Guido,Temp_Guido,HR_Sue,Temp_Sue)),
              index=ind, columns=col)
print(health_data)
```

## Indexing and Slicing - multi-indexed 

```{python}
#| echo: true
print(health_data)
```

Select a specific column on the first level 

`df['column name on level 1']`

```{python}
#| echo: true
print(health_data['Guido'])
```
## Indexing and Slicing - multi-indexed 

```{python}
#| echo: true
print(health_data)
```

equals to 

`df.loc[:,'column name on level 1']`
```{python}
#| echo: true
print(health_data.loc[:,'Guido'])
```
## Indexing and Slicing - multi-indexed 

```{python}
#| echo: true
print(health_data)
```

Select specific columns on the **first** and **second** levels 

`df['column name on level 1','column name on level 2']`

```{python}
#| echo: true
print(health_data['Guido', 'HR'])
```
## Indexing and Slicing - multi-indexed 

```{python}
#| echo: true
print(health_data)
```

equals to 

`df.loc[:,('column name on level 1','level 2')]`
```{python}
#| echo: true
print(health_data.loc[:,('Guido','HR')])
```
## Indexing and Slicing - multi-indexed 

```{python}
#| echo: true
print(health_data)
```

Multilevel slicing: complicated...

```{python}
#| echo: true
print(health_data.loc[:,('Guido', 'HR'):('Sue', 'HR')])
```

## Indexing and Slicing - multi-indexed 

```{python}
#| echo: true
print(health_data)
```

Select a specific column and **select rows**

`df.loc[row selection , column selection]`

```{python}
#| echo: true
print(health_data.loc[2013,'Guido'])
```
## Indexing and Slicing - multi-indexed 

```{python}
#| echo: true
print(health_data)
```

Select columns and rows, multi-level

`df.loc[(row selection lv1, lv2), (column selection lv1 ,lv2)]`

instead of using `:`, use `slice(None)` for selecting all contents
```{python}
#| echo: true
print(health_data.loc[(2013,slice(None)),(slice(None),'HR')])
```



## Sorting Indices

```{python}
#| echo: true
print(pop)
```

Sort index for proper slicing `.sort_index()`

```{python}
#| echo: true
pop_sort = pop.sort_index()
print(pop_sort)
```

## Stacking and Unstacking

```{python}
#| echo: true
print(pop)
```

Convert **Series** to **DataFrame** `.unstack()`

```{python}
#| echo: true
pop_df = pop.unstack()
print("unstack: ")
print(pop_df)
```



## Data Aggregations

```{python}
#| echo: true
print(health_data)
```
Default: collapse row
```{python}
#| echo: true
data_mean = health_data.mean()
print(data_mean)
```

## Data Aggregations

```{python}
#| echo: true
print(health_data)
```

Average by year with `.groupby()`

```{python}
#| echo: true
data_mean = health_data.groupby('year').mean()
print(data_mean)
```

## Data Aggregations

```{python}
#| echo: true
print(health_data)
```

Transpose for collapsing on row 

```{python}
#| echo: true
data_mean.T.mean() # just for demo
```

## Data Aggregations

```{python}
#| echo: true
print(health_data)
```

Average by measurement type across columns `.groupby()`

```{python}
#| echo: true
print(data_mean.T.groupby('type').mean())
```

## Hands-on - Hierarchical Indexing

```{python}
#| echo: true
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# Row index: Hospital, Department, Patient ID
# Column index: Metric Type, Measurement
hospitals = ['General Hospital', 'General Hospital', 'Community Clinic', 'Community Clinic']
departments = ['Cardiology', 'Neurology'] * 2
patient_ids = list(range(1001, 1009))  # 8 patients

# Create row multi-index
row_tuples = [(h, d, p) for h, d in zip(hospitals * 2, departments * 2) for p in [patient_ids.pop(0)]]
row_index = pd.MultiIndex.from_tuples(row_tuples, names=['Hospital', 'Department', 'PatientID'])

# Create column multi-index
metric_types = ['Vital Signs', 'Vital Signs', 'Lab Results', 'Lab Results']
measurements = ['Blood Pressure', 'Heart Rate', 'Glucose', 'Cholesterol']
col_index = pd.MultiIndex.from_arrays([metric_types, measurements], names=['Type', 'Measurement'])

# Generate sample data
np.random.seed(42)
data = np.random.randint(60, 180, size=(8, 4))
health_df = pd.DataFrame(data, index=row_index, columns=col_index)
```

## Hands-on - Hierarchical Indexing

```{python}
#| echo: true
print("Healthcare Patient Data:")
print(health_df)
```

## Hands-on - Hierarchical Indexing

-   Select all vital signs for patients in the Cardiology department
-   Calculate the average values for each measurement by hospital
-   Create a boolean mask to identify patients with heart rate \> 100 and glucose \> 120

## Pandas

-   Introduction to Pandas Objects
-   Data Indexing and Selection in Pandas
-   Operations in Pandas
-   Handling Missing Data in Pandas
-   Hierarchical Indexing in Pandas
-   **Combining Datasets: Concat and Append**
-   Combining Datasets: Merge and Join
-   Aggregation and Grouping in Pandas
-   Working with Time Series in Pandas

# Combining Datasets: Concat and Append

## Helper Function

```{python}
#| echo: true
def make_df(cols, ind):
    """Quickly make a DataFrame"""
    data = {c: [str(c) + str(i) for i in ind]
            for c in cols}
    return pd.DataFrame(data, ind)

class display(object):
    """Display HTML representation of multiple objects"""
    template = """<div style="float: left; padding: 10px;">
    <p style='font-family:"Courier New", Courier, monospace'>{0}</p>{1}
    </div>"""
    def __init__(self, *args):
        self.args = args
        
    def _repr_html_(self):
        return '\n'.join(self.template.format(a, eval(a)._repr_html_())
                         for a in self.args)
    
    def __repr__(self):
        return '\n\n'.join(a + '\n' + repr(eval(a))
                           for a in self.args)
```

## Basic Concatenation - DataFrames

`pd.concat()` Default: Concatenate vertically (`axis=0`)

```{python}
#| echo: true
df1 = make_df('AB', [1, 2]) 
df2 = make_df('AB', [3, 4])  
df12 = pd.concat([df1, df2])
display('df1', 'df2', 'df12')
```

## Concatenation - Axis Selection

You can concatenate column-wise by `axis=1`

```{python}
#| echo: true
df3 = make_df('AB', [0, 1])
df4 = make_df('CD', [0, 1])
df5 = pd.concat([df3, df4], axis=1) 
display('df3', 'df4', 'df5')
```

## Concatenation - Duplicate Indices

Pandas **preserves indices** during concatenation: can result in duplicate indices

```{python}
#| echo: true
x = make_df('AB', [0, 1])
y = make_df('AB', [0, 1])
display('x', 'y', 'pd.concat([x, y])')
```

## Handling Duplicate Indices

1.  **Verify integrity**: Raise an error if duplicates exist `verify_integrity=True`

```{python}
#| echo: true
try:
    pd.concat([x, y], verify_integrity=True)
except ValueError as e:
    print("ValueError:", e)
```

## Handling Duplicate Indices

2.  **Ignore indices**: Create new integer indices `ignore_index=True`

```{python}
#| echo: true
index2 = pd.concat([x, y], ignore_index=True)
display('x', 'y', "index2")
```

## Handling Duplicate Indices

3.  **Add hierarchical keys**: Create a MultiIndex to distinguish sources `keys=`

```{python}
#| echo: true
index3 = pd.concat([x, y], keys=['x', 'y'])
display('x', 'y', "index3")
```

## Concatenation

When concatenating DataFrames with shared columns

```{python}
#| echo: true
df5 = make_df('AB', [1, 2])
df6 = make_df('BC', [2, 3])
display('df5', 'df6', 'pd.concat([df5, df6])')
```

## Hands-on - Concat and Append

Here is sales data from different regional offices that need to be combined for company-wide analysis.

```{python}
#| echo: true
import pandas as pd
import numpy as np

# Create sales data for different regions
def create_sales_data(region, periods, seed=None):
    if seed is not None:
        np.random.seed(seed)
    
    dates = pd.date_range(start='2025-01-01', periods=periods)
    products = ['Laptop', 'Phone', 'Tablet', 'Monitor']
    
    data = {
        'Date': np.random.choice(dates, size=periods),
        'Product': np.random.choice(products, size=periods),
        'Units': np.random.randint(1, 50, size=periods),
        'Price': np.random.uniform(100, 1500, size=periods).round(2),
        'Region': region
    }
    
    df = pd.DataFrame(data)
    df['Revenue'] = df['Units'] * df['Price']
    return df

# Create regional datasets
north_sales = create_sales_data('North', 20, seed=42)
south_sales = create_sales_data('South', 15, seed=43)
east_sales = create_sales_data('East', 25, seed=44)
west_sales = create_sales_data('West', 18, seed=45)
```

## Hands-on - Concat and Append

```{python}
#| echo: true
# Display the first few rows of each dataset
print("North Region Sales:")
print(north_sales.head(3))
print("\nSouth Region Sales:")
print(south_sales.head(3))
```

## Hands-on - Concat and Append

-   Combine all regional sales data into a single DataFrame
-   Reset indices as 'Date' in original DataFrames, then perfrom default concatenation (preserves original indices)
-   When perform concatenation, adding hierarchical keys to track source

## Pandas

-   Introduction to Pandas Objects
-   Data Indexing and Selection in Pandas
-   Operations in Pandas
-   Handling Missing Data in Pandas
-   Hierarchical Indexing in Pandas
-   Combining Datasets: Concat and Append
-   **Combining Datasets: Merge and Join**
-   Aggregation and Grouping in Pandas
-   Working with Time Series in Pandas

# Combining Datasets: Merge and Join

## Relational Algebra in Pandas

Pandas provides high-performance, in-memory join and merge operations through the `pd.merge` function

1.  **One-to-one joins**: Combine datasets with unique keys in both DataFrames
2.  **Many-to-one joins**: Join when one DataFrame has duplicate keys
3.  **Many-to-many joins**: Join when both DataFrames have duplicate keys -\> **should be avoided**

## Basic Join Example

Using the common column as the key.

```{python}
#| echo: true
df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})
df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],'hire_date': [2004, 2008, 2012, 2014]})
dfmerge = pd.merge(df1, df2)
```

## Basic Join Example

```{python}
display('df1','df2')
```

## Basic Join Example

```{python}
display('dfmerge')
```

## Specifying Merge Keys

Using the `on` Parameter

```{python}
#| echo: true
pd.merge(df1, df2, on='employee')
```

## Different Column Names

`left_on` and `right_on` Parameter

```{python}
#| echo: true
df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
                    'salary': [70000, 80000, 120000, 90000]})
print(df1)
print(df3)
print(pd.merge(df1, df3, left_on="employee", right_on="name"))
```

## Merging on Index

```{python}
#| echo: true
df1a = df1.set_index('employee')
df2a = df2.set_index('employee')
```

The DataFrame `.join()` method provides a shortcut for **index-based** merges:

```{python}
#| echo: true
print(df1a.join(df2a))
```

## Join Types: Inner join

`how` Parameter: **Inner join** (default): Returns only matching rows

```{python}
#| echo: true
df7 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],
                    'food': ['fish', 'beans', 'bread']},
                   columns=['name', 'food'])
df8 = pd.DataFrame({'name': ['Mary', 'Joseph'],
                    'drink': ['wine', 'beer']},
                   columns=['name', 'drink'])
innerpd = pd.merge(df7, df8, how='inner')
```

::::: columns
::: column
```{python}
print('df7')
print(df7)
```
:::

::: column
```{python}
print('df8')
print(df8)
```
:::
:::::

```{python}
print('innerpd')
print(innerpd)
```

## Join Types: Outer join

`how` Parameter: **Outer join**: Returns all rows from both DataFrames, filling missing values with NaN

```{python}
#| echo: true
outerpd = pd.merge(df7, df8, how='outer')
```

::::: columns
::: column
```{python}
print('df7')
print(df7)
```
:::

::: column
```{python}
print('df8')
print(df8)
```
:::
:::::

```{python}
print('outerpd')
print(outerpd)
```

## Join Types: Left join

`how` Parameter: **Left join**: Returns all rows from the left DataFrame

```{python}
#| echo: true
leftpd = pd.merge(df7, df8, how='left')
```

::::: columns
::: column
```{python}
print('df7')
print(df7)
```
:::

::: column
```{python}
print('df8')
print(df8)
```
:::
:::::

```{python}
print('leftpd')
print(leftpd)
```

## Join Types: Right join

`how` Parameter: **Right join**: Returns all rows from the right DataFrame

```{python}
#| echo: true
rightpd = pd.merge(df7, df8, how='right')
```

::::: columns
::: column
```{python}
print('df7')
print(df7)
```
:::

::: column
```{python}
print('df8')
print(df8)
```
:::
:::::

```{python}
print('rightpd')
print(rightpd)
```

## Handling Duplicate Column Names

When DataFrames have conflicting column names, Pandas adds suffixes:

```{python}
#| echo: true
# Default suffixes (_x and _y)
df9 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
                    'rank': [1, 2, 3, 4]})
df10 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
                    'rank': [3, 1, 4, 2]})
pd.merge(df9, df10, on="name")
```

## Handling Duplicate Column Names

Custom suffixes with `suffixes` parameter

```{python}
#| echo: true
# Custom suffixes
pd.merge(df9, df10, on="name", suffixes=["_L", "_R"])
```

## Hands-on: US States Data

```{python}
#| echo: true
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
pop_url = "https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-population.csv"
area_url = "https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-areas.csv"
abb_url = "https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-abbrevs.csv"
pop = pd.read_csv(pop_url)
areas = pd.read_csv(area_url)
abbrevs = pd.read_csv(abb_url)
```

## Hands-on: US States Data

:::::: columns
::: column
```{python}
print('pop.head()')
print(pop.head())
```
:::

::: column
```{python}
print('areas.head()')
print(areas.head())
```
:::

::: column
```{python}
print('abbrevs.head()')
print(abbrevs.head())
```
:::
::::::

## Hands-on: US States Data

Merge multiple datasets to calculate population density:

1.  Merge population data `pop` with state abbreviations `abbrevs`
2.  Handle missing values and mismatches (?)
3.  Merge with area data `areas`
4.  Calculate and rank US states and territories by their 2010 population density

## Hint Q2 PR

```{python}
#| echo: true
print(pop.query("`state/region`	 == 'PR'").head())
```

Do we have PR in `abbrevs` table?

```{python}
#| echo: true
abbrevs.query("abbreviation == 'PR'")
```

## Hint for Q4 `.query()`

```{python}
#| echo: true
pop2010 = pop.query("year == 2010 & ages == 'total'")
pop2010.head()
```

## Hint for Q4 Sort

`df.sort_values(by='column name', ascending=False)`

```{python}
#| echo: true
pop2010.sort_values(by='population',ascending=False).head()
```

## Pandas

-   Introduction to Pandas Objects
-   Data Indexing and Selection in Pandas
-   Operations in Pandas
-   Handling Missing Data in Pandas
-   Hierarchical Indexing in Pandas
-   Combining Datasets: Concat and Append
-   Combining Datasets: Merge and Join
-   **Aggregation and Grouping in Pandas**
-   Working with Time Series in Pandas

# Aggregation and Grouping in Pandas

## Basic Aggregation

For DataFrames, aggregations operate on each column (**collapse rows**)

```{python}
#| echo: true
df = pd.DataFrame({'A': [0.16, 0.06, 0.87, 0.60, 0.71],
                   'B': [0.02, 0.97, 0.83, 0.21, 0.18]})
print(df)
df.mean()
```
## Basic Aggregation
Use axis parameter `axis='columns'` to aggregate across rows instead (**collapse columns**)

```{python}
#| echo: true
df.mean(axis='columns')
```

## Basic Aggregation `describe()`

quick statistical summary of your data: `describe()`

```{python}
#| echo: true
planets = sns.load_dataset('planets')
print(planets.dropna().describe())
```

## GroupBy: Split, Apply, Combine

`.groupby()` for more sophisticated data analysis by:

1.  **Splitting** data into groups based on some criteria

2.  **Applying** a function to each group independently

3.  **Combining** the results into a data structure

## GroupBy: Split, Apply, Combine

![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/03.08-split-apply-combine.png) [Source](https://jakevdp.github.io/PythonDataScienceHandbook/)

## GroupBy: Split, Apply, Combine

```{python}
#| echo: true
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],
                   'data': range(6)})
print(df)
```
```{python}
#| echo: true
print(df.groupby('key').sum())
```
## Working with GroupBy Objects

**Dispatch Methods**: Apply DataFrame methods to each group

```{python}
#| echo: true
df = planets.groupby('method')['year'].describe()
print(df)
```

## Advanced GroupBy Operations

```{python}
#| echo: true
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],
                   'data1': range(6),
                   'data2': rng.randint(0, 10, 6)},
                   columns = ['key', 'data1', 'data2'])
print(df)
```

## GroupBy Operations - Aggregation

**Aggregation**: Multiple aggregations at once.`df.groupby().aggregate(list)`

```{python}
#| echo: true
df.groupby('key').aggregate(['min', 'median', 'max'])
```

## GroupBy Operations - Aggregation

Different aggregations per column `df.groupby().aggregate(dict)`

-   `key`: column
-   `value`: aggregation method

```{python}
#| echo: true
df.groupby('key').aggregate({'data1': 'min', 'data2': 'max'})
```

## GroupBy Operations - Filtering

**Filtering**: Keep only groups that satisfy a condition 

`df.groupby().filter(boolen or function)`

```{python}
#| echo: true
def filter_func(x):
    return x['data2'].std() > 4
agg_df = df.groupby('key').filter(filter_func)
```

::::: columns
::: column
```{python}
print('df')
print(print(df))
```
:::

::: column
```{python}
print('agg_df')
print(print(agg_df))
```
:::
:::::

## GroupBy Operations - Transformation

**Transformation**: Apply a transformation that preserves the input shape


`df.groupby().transform(function)`

```{python}
#| echo: true
trans_df = df.groupby('key').transform(lambda x: x - x.mean())
```

::::: columns
::: column
```{python}
print('df')
print(print(df))
```
:::

::: column
```{python}
print('trans_df')
print(print(trans_df))
```
:::
:::::

## GroupBy Operations - Apply

**Apply**: Apply arbitrary functions to each group

`df.groupby().apply(function)`

```{python}
#| echo: true
def norm_by_data2(x):
    # x is a DataFrame of group values
    x['data1'] /= x['data2'].sum()
    return x
app_df = df.groupby('key').apply(norm_by_data2)
```

::::: columns
::: column
```{python}
print('df')
print(print(df))
```
:::

::: column
```{python}
print('app_df')
print(print(app_df))
```
:::
:::::

## Practical Example - Groupby

Analyzing planet discovery methods by decade

```{python}
#| echo: true
decade = 10 * (planets['year'] // 10)
decade = decade.astype(str) + 's'
planets['decade'] = decade
print(planets)
```

## Practical Example - Groupby

```{python}
#| echo: true
method_sum = planets.groupby(['method', 'decade'])['number'].sum()
print(method_sum)
```
## Practical Example - Groupby

`.unstack()`

```{python}
#| echo: true
unstack_df = method_sum.unstack()
print(unstack_df)
```
## Practical Example - Groupby

`.unstack().fillna(0)`

```{python}
#| echo: true
unstack_df = method_sum.unstack().fillna(0)
print(unstack_df)
```

## Pivot Tables in Pandas

Pivot tables are a powerful data analysis tool that provide multidimensional summarization of data. They can be thought of as a **multidimensional version** of GroupBy aggregation.

-   Allows for multiple levels of indices and columns
-   Supports various aggregation functions

## Example: Titanic Dataset

```{python}
#| echo: true
import pandas as pd
import seaborn as sns
titanic = sns.load_dataset('titanic')
print(titanic)
```

## Titanic Dataset - Basic pivot table

`df.pivot_table('column name', index=, columns=)`

Default: `aggfunc='mean'`

```{python}
#| echo: true
survival_by_class = titanic.pivot_table('survived', index='sex', columns='class')
print(survival_by_class)
```

## Titanic Dataset - Multi-level

`df.pivot_table('column name', index=, columns=)`

add multi-level index by `[level 1, level 2]`
```{python}
#| echo: true
age_groups = pd.cut(titanic['age'], [0, 18, 80])
survival_by_age_class = titanic.pivot_table('survived', 
                                            ['sex', age_groups], 
                                            'class')
print(survival_by_age_class)
```

## Additional Options

-   `aggfunc`: Specify aggregation function (e.g., 'sum', 'mean', 'count')
-   `fill_value`: Replace missing values
-   `margins`: Add row/column totals
-   `dropna`: Remove rows with missing values

## Practical Application: US Birth Data

```{python}
#| echo: true
births = pd.read_csv('https://raw.githubusercontent.com/jakevdp/data-CDCbirths/master/births.csv')
births['decade'] = 10 * (births['year'] // 10)
print(births.head())
```

## Practical Application: US Birth Data

Analyze births by decade and gender

```{python}
#| echo: true
births_by_decade = births.pivot_table('births', index='decade', 
                                      columns='gender', aggfunc='sum')
print(births_by_decade)
```

## Practical Application: US Birth Data

Visualize trends

```{python}
#| echo: true
births_by_decade.plot()
```

## Hands-on: Aggregation

```{python}
#| echo: true
import pandas as pd
# Download the data
births = pd.read_csv('https://raw.githubusercontent.com/jakevdp/data-CDCbirths/master/births.csv')
births['decade'] = 10 * (births['year'] // 10)
print(births.head())
```

## Hands-on: Aggregation

-   Group the data by gender and calculate the total number of births for each gender.
-   Group by both year and gender, then sum the number of births.
-   Filter out years where the total number of births (all genders combined) was less than 3 million.
-   Create a pivot table with decade as rows, gender as columns, and the sum of number of births as values.
-   Create a pivot table showing the average number of births per month for each gender.

## Pandas

-   Introduction to Pandas Objects
-   Data Indexing and Selection in Pandas
-   Operations in Pandas
-   Handling Missing Data in Pandas
-   Hierarchical Indexing in Pandas
-   Combining Datasets: Concat and Append
-   Combining Datasets: Merge and Join
-   Aggregation and Grouping in Pandas
-   **Working with Time Series in Pandas**

# Working with Time Series in Pandas

## Types of Time Data

Pandas provides extensive tools for working with time-based data, reflecting its origins in financial modeling:

-   **Time stamps**: Reference specific moments (e.g., July 4th, 2015 at 7:00am)
-   **Time intervals/periods**: Reference lengths of time between points (e.g., the year 2015)
-   **Time deltas/durations**: Reference exact time lengths (e.g., 22.56 seconds)

## Native Python (datetime)

`datetime(year,month,day)`

```{python}
#| echo: true
from datetime import datetime
datetime(year=2015, month=7, day=4)
```

## NumPy's datetime64

`np.array(string, dtype=np.datetime64)`
```{python}
#| echo: true
import numpy as np
date = np.array('2015-07-04', dtype=np.datetime64)
date + np.arange(12)  # Array of consecutive dates
```

## Pandas Time Objects

Pandas combines the ease-of-use of datetime with the efficiency of numpy.datetime64:

`pd.to_datetime(string)`

```{python}
#| echo: true
import pandas as pd
date = pd.to_datetime("4th of July, 2015")
print(date)
```
`.strftime(?)` for more information
```{python}
#| echo: true
date.strftime('%A')  # 'Saturday'
```
## Parameters for date.strftime

[Ref](https://strftime.org/)

| Code | Description | Example |
|--------------|--------------------------------------------|--------------|
| `%a` | Abbreviated weekday name | Sun, Mon, … |
| `%A` | Full weekday name | Sunday, Monday, … |
| `%w` | Weekday as a decimal number (0=Sunday, 6=Saturday) | 0, 1, …, 6 |
| `%d` | Day of the month (zero-padded) | 01, 02, …, 31 |
| `%b` | Abbreviated month name | Jan, Feb, …, Dec |
| `%B` | Full month name | January, February, … |
| `%m` | Month as a zero-padded decimal number | 01, 02, …, 12 |

## Pandas Time Series Data Structures

-   **Timestamp**: Replacement for Python's datetime
-   **DatetimeIndex**: Index structure for timestamps
-   **Period/PeriodIndex**: For fixed-frequency time intervals
-   **Timedelta/TimedeltaIndex**: For time durations

## Creating Date Ranges

Daily frequency (default) `pd.date_range(start date, end date (included))`

```{python}
#| echo: true
pd.date_range('2015-07-03', '2015-07-10')
```

Hourly frequency

`pd.date_range(start date, periods = #, freq='')`

```{python}
#| echo: true
pd.date_range('2015-07-03', periods=8, freq='h')
```

## Creating Date Ranges

Monthly periods

`pd.date_range(start date, periods = #, freq='')`

```{python}
#| echo: true
pd.period_range('2015-07', periods=8, freq='M')
```

## Time Series Indexing

`pd.DatetimeIndex(list of date)`
```{python}
#| echo: true
index = pd.DatetimeIndex(['2014-07-04', '2014-08-04', 
                          '2015-07-04', '2015-08-04'])
data = pd.Series([0, 1, 2, 3], index=index)
print(data)
```

## Time Series Indexing

Slice by date range

```{python}
#| echo: true
data['2014-07-04':'2015-07-04']
```

Slice by year

```{python}
#| echo: true
data['2015']
```

## Sample dataset

```{python}
#| echo: true
#!pip3 install yfinance
import yfinance as yf
dat = yf.Ticker("MSFT")
MSFT = dat.history(period='1y')
print(MSFT.head())
```

## Frequency and Offset Codes

Common frequency codes:

::::: columns
::: column
-   `D`: Calendar day
-   `B`: Business day
-   `W`: Weekly
-   `ME`: Month end
:::

::: column
-   `QE`: Quarter end
-   `YE`: Year end
-   `h`: Hours
-   `min`: Minutes
:::
:::::

## Time Series Operations - Resampling

Mean for each year `.resample(any resample freq).mean()`

```{python}
#| echo: true
year_m = MSFT.resample('YE').mean()  
print(year_m)
```

## Time Series Operations - Resampling

Mean for each Quarter/bussiness day `.resample(any resample freq).mean()`

```{python}
#| echo: true
qua_m = MSFT.resample('BQE').mean()  
print(qua_m)
```

## Time Series Operations - Resampling

Value at end of each year  `.asfreq(any freq)`

```{python}
#| echo: true
year_v = MSFT.asfreq('YE')  
print(year_v)
year_c = MSFT.resample('YE').count()  
print(year_c)
```

## Time Series Operations - Time-shifting

```{python}
#| echo: true
print(MSFT.head())
```

## Time Series Operations - Time-shifting
Shift data values `.shift(#)`, default is day
```{python}
#| echo: true
print(MSFT.shift(3).head())
```

## Time Series Operations - Rolling Windows

monthly Rolling mean

```{python}
#| echo: true
y_rolling = MSFT.rolling(30)
y_r_mean = y_rolling.mean().tail()
print(y_r_mean)
```

## Time Series Operations - Rolling Windows

weekly Rolling mean `.rolling(#)` default day

```{python}
#| echo: true
w_rolling = MSFT.rolling(7)
w_r_mean = w_rolling.mean().tail()  # 
print(w_r_mean)
```

## Hands-on - Time and Pandas

```{python}
#| echo: true
# Create a DataFrame with COVID-19 data for the last 5 days
dates = pd.date_range(end='2023-04-30', periods=5)
data = {
    'United States': [45000, 42000, 38000, 41000, 39000],
    'India': [12000, 13500, 14200, 13800, 12500],
    'Brazil': [9800, 9200, 8700, 9100, 9500],
    'France': [7500, 7200, 6800, 6500, 6200],
    'Germany': [5200, 4800, 4600, 4900, 5100]
}

daily_cases = pd.DataFrame(data, index=dates)
daily_cases.index.name = 'Date'
daily_cases.columns.name = 'Country'
print("Daily New COVID-19 Cases:")
print(daily_cases)
```

## Hands-on - Time and Pandas

-   Select and print the case numbers for India between '2023-04-27' and '2023-04-29' (inclusive).
-   Calculate the daily change (difference) in cases for Brazil.
-   Resample the data to a weekly frequency and calculate the mean for each country. (Hint: `.resample('W').mean()`)
-   Showing the 3-day moving average of cases for each country
-   For each country, identify the date with the highest number of new cases.
