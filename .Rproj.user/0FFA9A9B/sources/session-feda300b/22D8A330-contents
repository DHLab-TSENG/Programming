---
title: "3. Data I/O"
author: "Yi-Ju Tseng"
format:
  revealjs:
    slide-number: c/t
    show-slide-number: all
editor: visual
---

## 資料分析步驟

-   **資料匯入與匯出**
-   資料清洗處理
-   資料分析
-   資料呈現與視覺化

## 資料匯入與匯出

-   從檔案匯入
-   從Google drive匯入
-   從API匯入
-   網頁爬蟲
-   資料匯出

## 前置作業

為了成功從https (加密封包傳輸)下載資料，首先取消證書驗證

```{python}
#| echo: true
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
```

# 從檔案匯入

## 從檔案匯入

-   .csv 逗號分隔格式檔案
-   .xls Excel格式檔案
-   純文字資料 (無分隔)
-   其他格式

## .csv 逗號分隔格式檔案

-   .csv 為逗號分隔格式檔案
-   可用`pandas`的`read_csv(檔案路徑)`功能將檔案匯入
-   檔案路徑可以是本機端路徑，也可以是網址

```{python}
#| echo: true
import pandas as pd
url = "https://quality.data.gov.tw//dq_download_csv.php?nid=102775&md5_url=ea56d6e1f2642b2c5c44f9e8b6185d54"
df_csv = pd.read_csv(url)
df_csv.head()
```

## .csv 逗號分隔格式檔案

```{python}
df_csv.head()
```

## Hands-on - .csv檔案匯入

-   以[上市公司每月營業收入彙總表](https://data.gov.tw/dataset/18420)為例
-   複製`csv`資料下載網址
-   用pandas 的 `read_csv()`函數將檔案匯入
-   匯入後印出前5個row

![](figures/downloadcsv.png)

## .xls Excel格式檔案

-   需要安裝`openpyxl`和`xlrd`套件

```{python}
#| echo: true
#| eval: false
!pip3 install openpyxl xlrd
```

## .xls Excel格式檔案

-   使用`pandas`套件中的`read_excel(檔案路徑)`功能
-   檔案路徑可以是本機端路徑，也可以是網址
-   可設定`sheet_name`參數指定工作表名稱

```{python}
#| echo: true
url = 'https://github.com/CGUIM-BigDataAnalysis/BigDataCGUIM/raw/master/EMBA_BigData/Data/%E6%96%B0%E7%AB%B9%E4%B8%8D%E5%8B%95%E7%94%A2.xls'
house = pd.read_excel(url,sheet_name = "不動產買賣")  
house.head()
```

## .xls Excel格式檔案

```{python}
house.head()
```

## Hands-on - .xls Excel格式檔案

-   以[雲林縣水位雨量站](https://data.gov.tw/dataset/145466)為例
-   複製`Excel`資料下載網址
-   用pandas 的 `read_excel()`函數將檔案匯入，記得設定工作表名稱
-   匯入後印出第三個row

## 純文字資料

-   使用`oepn(檔名)`開啟檔案
-   使用`檔案物件.read()`讀取內容。**一次取全部，不換行**
-   讀取完畢後，使用`檔案物件.close()`關閉檔案

```{python}
#| echo: true
f = open("output_write.txt")
print(f.read())
f.close()
```

## 純文字資料

-   使用`oepn(檔名)`開啟檔案
-   使用`檔案物件.readlines()`讀取內容。**一次取全部，分行存成list**
-   讀取完畢後，使用`檔案物件.close()`關閉檔案

```{python}
#| echo: true
f = open("output_write.txt")
print(f.readlines())
f.close()
```

## 純文字資料

-   使用`oepn(檔名)`開啟檔案
-   使用`檔案物件.readline()`讀取內容。**逐行讀**
-   讀取完畢後，使用`檔案物件.close()`關閉檔案

```{python}
#| echo: true
f = open("output_write.txt")
print(f.readline())
print(f.readline())
f.close()
```

## 其他格式

-   以圖片為例，需安裝並載入繪圖用套件`matplotlib`
-   使用`plt.imread(圖片檔案)`載入圖片，為RGB向量

```{python}
#| echo: true
#| eval: false
!pip3 install matplotlib
```

```{python}
#| echo: true
import matplotlib
import matplotlib.pyplot as plt
im = plt.imread('figures/cgu_logo.png') 
print(im)
plt.imshow(im, 'gray')
plt.show()
```

## 其他格式

-   使用`plt.imshow(向量)`呈現圖片
-   `plt.show()`在視窗中呈現圖片內容

```{python}
#| echo: true
plt.imshow(im, 'gray')
plt.show()
```

# 從Google drive匯入

## 從Google drive匯入 - google 試算表

-   可直接用`pandas`套件
-   注意原始連結後方，須為文件ID，若有`/edit#gid=0`或`/edit?usp=sharing`字樣，請刪除
-   需要加工連結，在原始連結最後加上`/export?format=csv`
-   如果不是公開資料表，需要授權，可參考`google-api-python-client`套件

```{python}
#| echo: true
link="https://docs.google.com/spreadsheets/d/1B7tTfBZ6DKIV-OH6rQEz3J1Poh928r08WqAqiIdVeqM"
csv_link="/export?format=csv"
pd.read_csv(link+csv_link)
```

## 從Google drive匯入 - google 試算表

```{python}
pd.read_csv(link+csv_link)
```

## Hands-on - google 試算表匯入

-   以[Example Spreadsheet](https://docs.google.com/spreadsheets/d/1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms)為例
-   加工連結 (**hint**:上一頁投影片)
-   用pandas 的 `pd.read_csv()`函數將檔案匯入
-   匯入後印出第Home State column

# 從API匯入

## 從API匯入

-   Open Data
-   API (Application programming interfaces)
-   JSON格式檔案

## Open Data 開放資料

-   2011年推動開放政府與開放資料 ([維基百科](https://zh.wikipedia.org/wiki/%E9%96%8B%E6%94%BE%E8%B3%87%E6%96%99))
-   不受著作權、專利權，以及其他管理機制所限制，任何人都可以自由出版使用
-   常見的儲存方式為:
    -   `CSV`
    -   `JSON`
    -   `XML`

## Open Data 開放資料常見平台

-   [政府資料開放平台](https://data.gov.tw/)
-   [台北市資料大平台](https://data.taipei/)
-   [桃園開放資料](https://data.tycg.gov.tw/)

## API

-   應用程式介面
-   **A**pplication **P**rogramming **I**nterfaces
-   為了讓第三方的開發者可以額外開發應用程式來強化他們的產品，推出可以與系統溝通的介面
-   有API輔助可將資料擷取過程自動化
    -   以下載Open Data為例，若檔案更新頻繁，使用手動下載相當耗時
-   [維基百科](https://zh.wikipedia.org/zh-tw/%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E6%8E%A5%E5%8F%A3)

## API - Open Data

::::: columns
::: column
-   [桃園公共自行車即時服務資料](http://data.tycg.gov.tw/opendata/datalist/datasetMeta?oid=5ca2bfc7-9ace-4719-88ae-4034b9a5a55c)資料
-   每日更新
-   不可能每日手動下載
-   提供透過**API**下載的服務
-   透過API下載的資料格式: **JSON格式**
:::

::: column
-   [桃園公共自行車即時服務資料API資訊](http://data.tycg.gov.tw/opendata/datalist/datasetMeta/outboundDesc?id=5ca2bfc7-9ace-4719-88ae-4034b9a5a55c&rid=a1b4714b-3b75-4ff8-a8f2-cc377e4eaa0f)
    -   **資料集ID**: 紀錄資料的基本參數，如包含欄位、更新頻率等
    -   **資料RID**: 資料集
    -   擷取範例
:::
:::::

## JSON格式檔案

-   JSON (**J**ava**s**cript **O**bject **N**otation)
-   輕量級的資料交換語言
-   From **a**pplication **p**rogramming **i**nterfaces (APIs)
-   JavaScript、Java、Node.js應用
-   一些NoSQL資料庫用JSON儲存資料：**MongoDB**
-   [Wiki](http://en.wikipedia.org/wiki/JSON)

![](figures/json.png){width="383"}

## JSON檔案匯入

-   從網路載資料，需要安裝`requests`套件
-   處理json資料，需要安裝與載入`json`套件

```{python}
#| echo: true
#| eval: false
!pip3 install json requests
```

```{python}
#| echo: true
import json, requests
```

## JSON檔案匯入

-   API網址參考[臺中市公共自行車(YouBike2.0)](https://opendata.taichung.gov.tw/search/6e38eb56-0e9a-4b9e-806d-23cd35d44d6b)
-   點選API獲取相關資訊
-   使用`requests`套件的`get(網址)`擷取資料
-   本API好像會檔短時間內擷取多次，因此放入`params = {'param':'1'}`以及`headers = {'Connection':'close'}`設定
-   最後將使用`回傳結果.json()`，將檔案轉成JSON格式

```{python}
#| echo: true
#| cache: true
url="https://datacenter.taichung.gov.tw/swagger/OpenData/86dfad5c-540c-4479-bb7d-d7439d34eeb1"
response = requests.get(url, params = {'param':'1'}, headers = {'Connection':'close'})
data = response.json()  # Convert the response to JSON
```

## JSON檔案匯入

探索資料，使用`.keys()`查看keys

```{python}
#| echo: true
type(data)
```

```{python}
#| echo: true
data.keys()
```

```{python}
#| echo: true
type(data["retVal"])
```

## JSON檔案匯入

探索資料，發現是dist (data)中retVal key儲存含有資料的list

```{python}
#| echo: true
data["retVal"][0:2]
```

## JSON檔案匯入

探索資料，發現是dist (data)中retVal key儲存含有資料的list

```{python}
#| echo: true
data["retVal"][0]
```

## Hands-on - JSON檔案匯入練習

-   練習用資料：[YouBike2.0臺北市公共自行車即時資訊](https://data.gov.tw/dataset/137993)
-   API地址[在此](https://tcgbusfs.blob.core.windows.net/dotapp/youbike/v2/youbike_immediate.json)
-   使用檔案匯入**範例**，將資料匯入Python中
    -   提示：設定url
-   找到資料的儲存處，並將答案print出

## 從API匯入 - Recap

-   Open Data
-   API (Application programming interfaces)
-   JSON格式檔案 `requests.get()`下載，`.json`轉型
-   用`type()`查看資料類別，`keys()`查看資料擷取方法，並印出來試試看

# 網頁爬蟲 Webscraping

## 網頁爬蟲 Webscraping

-   不是每個網站都提供API
-   人工複製貼上?!
-   程式化的方式擷取網頁資料: **網頁爬蟲（Webscraping）**（[Webscraping Wiki](http://en.wikipedia.org/wiki/Web_scraping)）
-   可能耗費很多網頁流量和資源 －很可能被鎖IP
-   使用`beautifulsoup4` 套件輔助

## 網頁爬蟲 Webscraping-beautifulsoup4

需要先安裝`beautifulsoup4`以及`requests`套件。

```{python}
#| echo: true
#| eval: false
!pip3 install beautifulsoup4
```

-   擷取條件的撰寫會因網頁語法不同而有差異
-   使用**Google Chrome開發工具**輔助觀察擷取資料的條件
    -   或是按右鍵，點選“檢查”
    -   或使用**SelectorGadget**輔助
-   觀察需要擷取的資料所在HTML片段
    -   css class, id 等

## 即時股價爬取

-   [Yahoo 股市](https://tw.stock.yahoo.com/)
-   [Yahoo 股市 - 台積電](https://tw.stock.yahoo.com/quote/2330)

```{python}
#| echo: true
from bs4 import BeautifulSoup
import requests
StockUrl = "https://tw.stock.yahoo.com/quote/2330"
res = requests.get(StockUrl)
soup = BeautifulSoup(res.text, "html.parser")
```

```{python}
#| echo: true
price = soup.select(".Fz\(32px\)")
price[0].get_text()
```

## 即時股價爬取 - 下載html

-   使用`requests`套件的`get(網址)`函數，輸入網址，將網頁載入python分析環境。
-   成功載入網頁後，針對該物件，使用`.text`取得網頁原始碼 (.html)

```{python}
#| echo: true
StockUrl = "https://tw.stock.yahoo.com/quote/2330"
res = requests.get(StockUrl)
res.text[0:1000] ## print first 1000 chars
```

## 即時股價爬取 - 解析html

-   使用`BeautifulSoup`套件提供的解析方法`BeautifulSoup(html檔案,格式)`
    -   輸入剛剛載回來的html檔案`res.text`，以及設定格式`"html.parser"`，完成HTML解析
-   解析後可使用`解析後內容.prettify`取得排版後的網頁原始碼

```{python}
#| echo: true
soup = BeautifulSoup(res.text, "html.parser")
```

## 即時股價爬取 - 搜尋所需節點 -1

-   `find(節點條件)`:
    -   取得第一個符合條件的節點 (`<>`內的內容)
-   `find_all(節點條件)`
    -   取得所有符合條件的節點 (`<>`內的內容)
-   `select_one()`
    -   取得第一個符合CSS條件的節點 (`class` or `id`等屬性)
-   `select()`
    -   取得所有符合CSS條件的節點 (`class` or `id`等屬性)

## 即時股價爬取 - 搜尋所需節點 -1

範例：class 包含 `Fz\(32px\)`

-   `select()`
    -   取得所有符合CSS條件的節點 (`class` or `id`等屬性)
-   `class=`可用`.`代表
-   `id=`可用`#`代表

```{python}
#| echo: true
soup = BeautifulSoup(res.text, "html.parser")
price = soup.select(".Fz\(32px\)")
price
```

## 即時股價爬取 - 取出需要的資料 -1

-   `.get_text()`可抓取節點內文字
-   `.get("屬性名稱")`可抓取屬性內容

```{python}
#| echo: true
price[0].get_text()
```

## 即時股價爬取 - 搜尋所需節點 -2

範例：class 包含 `price-detail-item`

-   `select()`
    -   取得所有符合CSS條件的節點 (`class` or `id`等屬性)

```{python}
#| echo: true
soup = BeautifulSoup(res.text, "html.parser")
price_detail = soup.select(".price-detail-item")
price_detail
```

## 即時股價爬取 - 取出需要的資料 -2

-   `.get_text()`可抓取節點內文字
-   `.get("屬性名稱")`可抓取屬性內容

```{python}
#| echo: true
len(price_detail)
```

```{python}
#| echo: true
for price in price_detail:
  print(price.get_text())
```

## 即時股價爬取 - 搜尋所需節點 -3

範例：class 包含 `StretchedBox`

-   `select()`: 取得所有符合CSS條件的節點 (`class` or `id`等屬性)

```{python}
#| echo: true
soup = BeautifulSoup(res.text, "html.parser")
news = soup.select(".mega-item-header-link")
news
```

## 即時股價爬取 - 取出需要的資料 -3

-   `.get_text()`可抓取節點內文字
-   `.get("屬性名稱")`可抓取屬性內容

```{python}
#| echo: true
len(news)
```

```{python}
#| echo: true
for title_click in news:
  print(title_click.get_text())
```

## Hands-on 爬蟲練習

-   [Ptt Tech_Job 版](https://www.ptt.cc/bbs/Tech_Job/index.html)
-   試著爬出所有**標題**
-   爬出的第三個標題是？
-   提示：
    -   下載html `requests.get(網址)`
    -   解析html `BeautifulSoup(下載網頁.text, "html.parser")`\
    -   搜尋所需節點 `find(節點)`, `select(css條件)`
    -   取出需要的資料 `get_text()`, `get(屬性名稱)`

## 即時股價爬取 - 取出需要的資料 -4

-   `.get_text()`可抓取節點內文字
-   `.get("屬性名稱")`可抓取屬性內容

```{python}
#| echo: true
for title_click in news:
  print(title_click.get("href"))
```

## 即時股價爬取 - 爬連結內的新聞

-   得到連結後，重複上述步驟
    -   下載html `requests.get(網址)`
    -   解析html `BeautifulSoup(下載網頁.text, "html.parser")`\
    -   搜尋所需節點 `find(節點)`, `select(css條件)`
    -   取出需要的資料 `get_text()`, `get(屬性名稱)`

## 即時股價爬取 - 爬連結內的新聞

首先取得連結list。

補充內容：

-   `sequence物件.append()`可新增內容至sequence中

```{python}
#| echo: true
url_list = []
title_list = []
for title_click in news:
  url_list.append(title_click.get("href"))
  title_list.append(title_click.get_text())
print(url_list)
```

## 即時股價爬取 - 爬連結內的新聞

以第一個新聞連結為例，下載html後，重複步驟，取得本文

```{python}
#| echo: true
res = requests.get(url_list[0])
soup = BeautifulSoup(res.text, "html.parser")
news_full = soup.select_one(".caas-body")
print("標題: "+title_list[0] +", 內文:" + news_full.get_text())
```

## 即時股價爬取 - 爬連結內的新聞

用`for`迴圈一次做完所有連結

```{python}
#| echo: true
for title_click in news:
  title = title_click.get_text()
  url = title_click.get("href")
  res = requests.get(url)
  soup = BeautifulSoup(res.text, "html.parser")
  news_full = soup.select_one(".caas-body")
  print("標題: "+title +", 內文:" + news_full.get_text())
```

## 即時股價爬取 - 爬連結內的新聞

用`for`迴圈一次做完所有連結，並轉成pandas data frame

```{python}
#| echo: true
newdf = []
for title_click in news:
  title = title_click.get_text()
  url = title_click.get("href")
  res = requests.get(url)
  soup = BeautifulSoup(res.text, "html.parser")
  news_full = soup.select_one(".caas-body")
  add_news = pd.DataFrame({"title":[title],"content":[news_full.get_text()]})
  newdf.append(add_news)

newdf_pd=pd.concat(newdf)
newdf_pd
```

## 即時股價爬取 - 爬連結內的新聞

```{python}
newdf_pd
```

## Hands-on 爬蟲練習

-   [Ptt Tech_Job 版](https://www.ptt.cc/bbs/Tech_Job/index.html)
-   試著爬出**最新的一頁**、**前一頁**以及**前前一頁**的所有**標題**和**內文網址**
-   總共爬出幾個標題？
-   提示：
    -   下載html `requests.get(網址)`
    -   解析html `BeautifulSoup(下載網頁.text, "html.parser")`\
    -   搜尋所需節點 `find(節點)`, `select(css條件)`
    -   取出需要的資料 `get_text()`, `get(屬性名稱)`
-   一頁一頁爬 or for 迴圈都可以

## 網頁爬蟲 進階版

-   滾動式等動態網頁網頁，如剛剛的新聞
    -   [Python 進階爬蟲 by Andy Chiang](https://hackmd.io/@AndyChiang/DynamicCrawler)

## 網頁爬蟲 再想想？

-   其實... 很多資料有其他存取方式，像API
-   隱私問題 （OkCupid事件）
    -   [70,000 OkCupid Users Just Had Their Data Published](https://motherboard.vice.com/en_us/article/70000-okcupid-users-just-had-their-data-published)

## 網頁爬蟲 Webscraping - Recap

-   下載html `requests.get(網址)`
-   解析html `BeautifulSoup(下載網頁.text, "html.parser")`
-   搜尋所需節點 `find(節點)`, `select(css條件)`
-   取出需要的資料 `get_text()`, `get(屬性名稱)`

# 資料匯出

## 資料匯出

-   文字檔 .txt
-   CSV檔 .csv
-   json檔 .json

## 資料匯出 - 文字檔 .txt

-   首先使用`open(檔案名稱)`開啟檔案
-   使用`檔案.writelines(資料)`寫入內容
-   `檔案.writelines(資料)`的資料可以是列表

```{python}
#| echo: true
path = 'output_writeline.txt'
f = open(path, 'w')
lines1 = ['Hello World', '123', '456', '789']
lines2 = ['Hello World\n', '123\n', '456\n', '789\n']
f.writelines(lines1)
f.writelines(lines2)
f.close()
```

## 資料匯出 - 文字檔 .txt

-   首先使用`open(檔案名稱)`開啟檔案
-   使用`檔案.write(資料)`寫入內容
-   `檔案.write(資料)`的資料必須是文字

```{python}
#| echo: true
path = 'output_write.txt'
f = open(path, 'w')
for t in lines1:
  f.write(t)
  
for t in lines2:
  f.write(t)

f.close()
```

## 資料匯出 - CSV檔 .csv

使用`pandas` Data frame物件中的`.to_csv(檔名)`儲存檔案。

```{python}
#| echo: true
newdf_pd.to_csv("news.csv")
```

## 資料匯出 - json檔 .json

使用`pandas` Data frame物件中的`.to_json(檔名)`儲存檔案。

DataFrame中一定要有index

```{python}
#| echo: true
newdf_pd = newdf_pd.set_index('title')
newdf_pd.to_json("news_json.csv")
newdf_pd
```

## Hands-on 資料匯出

-   [Ptt Tech_Job 版](https://www.ptt.cc/bbs/Tech_Job/index.html)
-   試著爬出最新一頁的所有**標題**
-   將標題存成一個資料框
-   儲存標題資料框，檔名為ptt.csv

## 資料匯出 - Recap

-   文字檔 .txt `writelines(內容)`
-   CSV檔 .csv `pandas_df.to_csv(檔名)`
-   json檔 .json `pandas_df.to_json(檔名)`
