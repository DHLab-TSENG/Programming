!pip3 install matplotlib
#| echo: true
from bs4 import BeautifulSoup
import requests
StockUrl = "https://www.google.com/finance/quote/TSMC:BCBA?hl=zh-TW"
res = requests.get(StockUrl)
soup = BeautifulSoup(res.text, "html.parser")
#| echo: true
price = soup.select(".fxKbKc")
price[0].get_text()
#| echo: true
from bs4 import BeautifulSoup
import requests
StockUrl = "https://www.google.com/finance/quote/2330:TPE?hl=zh-TW"
res = requests.get(StockUrl)
soup = BeautifulSoup(res.text, "html.parser")
#| echo: true
price = soup.select(".fxKbKc")
price[0].get_text()
#| echo: true
price = soup.select(".Yfwt5")
price[0].get_text()
#| echo: true
price = soup.select(".z4rs2b")
price[0].get_text()
#| echo: true
StockUrl = "https://www.google.com/finance/quote/2330:TPE?hl=zh-TW"
res = requests.get(StockUrl)
res.text[0:1000] ## print first 1000 chars
#| echo: true
soup = BeautifulSoup(res.text, "html.parser")
#| echo: true
soup = BeautifulSoup(res.text, "html.parser")
price = soup.select(".fxKbKc")
price
#| echo: true
price = soup.select(".z4rs2b")
price[0].get_text()
#| echo: true
soup = BeautifulSoup(res.text, "html.parser")
price = soup.select(".fxKbKc")
price
#| echo: true
price[0].get_text()
#| echo: true
soup = BeautifulSoup(res.text, "html.parser")
price_detail = soup.select(".gyFHrc")
price_detail
#| echo: true
len(price_detail)
#| echo: true
for price in price_detail:
print(price.get_text())
#| echo: true
price = soup.select(".z4rs2b")
price[0].get_text()
#| echo: true
price = soup.select(".z4rs2b")
price[0].get_text()
price[0].get("href")
#| echo: true
price = soup.select(".z4rs2b")
price[0].get("href")
#| echo: true
price = soup.select(".z4rs2b")
price[0].get_text()
price[0].get("href")
#| echo: true
price = soup.select(".z4rs2b")
price[0].get_text()
price[0].get("herf")
#| echo: true
news = soup.select(".z4rs2b")
news[0].get_text()
news
#| echo: true
news = soup.select(".z4rs2b")
news[0].get_text()
news[0]
#| echo: true
news = soup.select(".z4rs2b")
news[0].get_text()
news[0].get("href")
#| echo: true
news = soup.select(".z4rs2b")
#news[0].get_text()
news[0].get("href")
#| echo: true
news = soup.select(".z4rs2b")
#news[0].get_text()
print(news[0].get("href"))
#| echo: true
news = soup.select(".z4rs2b")
#news[0].get_text()
print(news[0].find("a").get("href"))
#| echo: true
news = soup.select(".z4rs2b")
#news[0].get_text()
news[0].find("a").get("href")
#| echo: true
news = soup.select(".z4rs2b")
news[0].get_text()
news[0].find("a").get("href")
#| echo: true
len(news)
#| echo: true
for title_click in news:
print(title_click.get_text())
#| echo: true
for title_click in news:
print(title_click.find("a").get("href"))
#| echo: true
url_list = []
title_list = []
for title_click in news:
url_list.append(title_click.get("href"))
title_list.append(title_click.get_text())
print(url_list)
#| echo: true
url_list = []
title_list = []
for title_click in news:
url_list.append(title_click.find("a").get("href"))
title_list.append(title_click.get_text())
print(url_list)
#| echo: true
res = requests.get(url_list[0])
soup = BeautifulSoup(res.text, "html.parser")
news_full = soup.select_one(".caas-body")
print("標題: "+title_list[0] +", 內文:" + news_full.get_text())
#| echo: true
res = requests.get(url_list[0])
soup = BeautifulSoup(res.text, "html.parser")
news_full = soup.select("p")
print("標題: "+title_list[0] +", 內文:" + news_full.get_text())
#| echo: true
res = requests.get(url_list[0])
soup = BeautifulSoup(res.text, "html.parser")
news_full = soup.select("p")
full_text = ""
for news_para in news_full:
full_text = full_text + news_para.get_text()
print("標題: "+title_list[0] +", 內文:" + full_text)
#| echo: true
res = requests.get(url_list[1])
soup = BeautifulSoup(res.text, "html.parser")
news_full = soup.select("p")
full_text = ""
for news_para in news_full:
full_text = full_text + news_para.get_text()
print("標題: "+title_list[0] +", 內文:" + full_text)
#| echo: true
res = requests.get(url_list[1])
soup = BeautifulSoup(res.text, "html.parser")
news_full = soup.select("p")
full_text = ""
for news_para in news_full:
full_text = full_text + news_para.get_text()
print("標題: "+title_list[1] +", 內文:" + full_text)
#| echo: true
newdf = []
for title_click in news:
title = title_click.get_text()
url = title_click.get("href")
res = requests.get(url)
soup = BeautifulSoup(res.text, "html.parser")
news_full = soup.select("p")
full_text = ""
for news_para in news_full:
full_text = full_text + news_para.get_text()
add_news = pd.DataFrame({"title":[title],"content":[full_text)]})
newdf.append(add_news)
newdf_pd=pd.concat(newdf)
newdf_pd
#| echo: true
newdf = []
for title_click in news:
title = title_click.get_text()
url = title_click.find("a").get("href")
res = requests.get(url)
soup = BeautifulSoup(res.text, "html.parser")
news_full = soup.select("p")
full_text = ""
for news_para in news_full:
full_text = full_text + news_para.get_text()
add_news = pd.DataFrame({"title":[title],"content":[full_text)]})
newdf.append(add_news)
newdf_pd=pd.concat(newdf)
newdf_pd
#| echo: true
newdf = []
for title_click in news:
title = title_click.get_text()
url = title_click.find("a").get("href")
res = requests.get(url)
soup = BeautifulSoup(res.text, "html.parser")
news_full = soup.select("p")
full_text = ""
for news_para in news_full:
full_text = full_text + news_para.get_text()
add_news = pd.DataFrame({"title":[title],"content":[full_text]})
newdf.append(add_news)
newdf_pd=pd.concat(newdf)
newdf_pd
reticulate::repl_python()
