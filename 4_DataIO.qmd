---
title: "4. Data I/O"
author: "Yi-Ju Tseng"
format:
  revealjs:
    slide-number: c/t
    show-slide-number: all
editor: visual
---

## Data input and output

-   Data input
    -   From file
    -   From google drive
    -   From API
    -   From database
    -   Webscrapping
-   Data output

## 前置作業

為了成功從https (加密封包傳輸)下載資料，首先取消證書驗證

```{python}
#| echo: true
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
```

## Data input and output

-   **Data input**
    -   **From file**
    -   From google drive
    -   From API
    -   From database
    -   Webscrapping
-   Data output

# From File

## From File

-   .csv 逗號分隔格式檔案
-   .xls Excel格式檔案
-   純文字資料
-   其他格式

## .csv 逗號分隔格式檔案

-   .csv 為逗號分隔格式檔案
-   `pandas`的`read_csv(路徑)`，將檔案匯入
-   路徑可以是*本機端路徑*，也可以是*網址*

```{python}
#| echo: true
import pandas as pd
url = "https://quality.data.gov.tw//dq_download_csv.php?nid=102775&md5_url=ea56d6e1f2642b2c5c44f9e8b6185d54"
df_csv = pd.read_csv(url)
df_csv.head()
```

## .csv 逗號分隔格式檔案

```{python}
df_csv.head()
```

## Hands-on - .csv檔案匯入

-   以[上市公司每月營業收入彙總表](https://data.gov.tw/dataset/18420)為例
-   複製`csv`資料下載網址
-   用`pandas` 的 `read_csv()`函數將檔案匯入
-   匯入後印出前5個row

![](figures/downloadcsv.png)

## .xls Excel格式檔案

-   需要安裝`openpyxl`和`xlrd`套件

```{python}
#| echo: true
#| eval: false
!pip3 install openpyxl xlrd
```

## .xls Excel格式檔案

-   使用`pandas`的`read_excel(路徑)`
-   路徑可以是*本機端路徑*，也可以是*網址*
-   可設定`sheet_name`參數指定*工作表名稱*

```{python}
#| echo: true
url = 'https://github.com/CGUIM-BigDataAnalysis/BigDataCGUIM/raw/master/EMBA_BigData/Data/%E6%96%B0%E7%AB%B9%E4%B8%8D%E5%8B%95%E7%94%A2.xls'
house = pd.read_excel(url,sheet_name = "不動產買賣")  
house.head()
```

## .xls Excel格式檔案

```{python}
house.head()
```

## Hands-on - .xls Excel格式檔案

-   以[水位雨量站](https://data.gov.tw/dataset/145466)為例
-   複製`Excel`資料下載網址
-   用pandas 的 `read_excel()`函數將檔案匯入，記得設定工作表名稱
-   匯入後印出第三個row

## 純文字資料

-   使用`oepn(檔名)`開啟檔案
    - 以sequence形式儲存
-   使用`檔案物件.read()`讀取內容。**一次取全部**
-   讀取完畢後，使用`檔案物件.close()`關閉檔案

```{python}
#| echo: true
f = open("output_write.txt")
print(f.read())
f.close()
```

## 純文字資料

-   使用`oepn(檔名)`開啟檔案
-   使用`檔案物件.readlines()`讀取內容。**一次取全部，分行存成list**
-   讀取完畢後，使用`檔案物件.close()`關閉檔案

```{python}
#| echo: true
f = open("output_write.txt")
print(f.readlines())
f.close()
```

## 純文字資料

-   使用`oepn(檔名)`開啟檔案
-   使用`檔案物件.readline()`讀取內容。**逐行讀**
-   讀取完畢後，使用`檔案物件.close()`關閉檔案

```{python}
#| echo: true
f = open("output_write.txt")
print(f.readline())
print(f.readline())
f.close()
```

## dist + for + open
```{python}
#| echo: true
name = 'trump.txt'
handle = open(name)

counts = dict()
for line in handle:
    words = line.split()
    for word in words:
        counts[word] = counts.get(word,0) + 1

bigcount = None
bigword = None
for word,count in counts.items():
    if bigcount is None or count > bigcount:
        bigword = word
        bigcount = count

print(bigword, bigcount)
```

## 其他格式

-   以圖片為例，需安裝並載入繪圖用套件`matplotlib`
-   使用`plt.imread(路徑)`載入圖片，為RGB向量

```{python}
#| echo: true
#| eval: false
!pip3 install matplotlib
```

```{python}
#| echo: true
import matplotlib
import matplotlib.pyplot as plt
im = plt.imread('figures/nycu_logo.png') 
print(im)
plt.imshow(im, 'gray')
plt.show()
```

## 其他格式

-   使用`plt.imshow(向量)`呈現圖片
-   `plt.show()`在視窗中呈現圖片內容

```{python}
#| echo: true
plt.imshow(im, 'gray')
plt.show()
```

## Data input and output

-   **Data input**
    -   From file
    -   **From google drive**
    -   From API
    -   From database
    -   Webscrapping
-   Data output

# 從Google drive匯入

## 從Google drive匯入 - google 試算表

-   `pandas`套件
-   注意原始連結後方，須為文件ID
    -   若有`/edit#gid=0`或`/edit?usp=sharing`字樣，請刪除
-   需要加工連結，在原始連結最後加上`/export?format=csv`
-   如果不是公開資料表，需要授權
    -   參考`google-api-python-client`套件

```{python}
#| echo: true
link="https://docs.google.com/spreadsheets/d/1WqkYGfZZinxqhyqnbHIDC4NrHXbApfm8m8EvFR4SPAU"
csv_link="/export?format=csv"
pd.read_csv(link+csv_link)
```

## 從Google drive匯入 - google 試算表

```{python}
pd.read_csv(link+csv_link)
```

## Hands-on - google 試算表匯入

-   以[Example Spreadsheet](https://docs.google.com/spreadsheets/d/1wmcc6e8n9bvIQgps6MgFLyLCcq6DWx-3uP2uLIA9rnU)為例
-   加工連結 (**hint**:前兩頁投影片)
-   用pandas 的 `pd.read_csv()`函數將檔案匯入
-   匯入後印出第Home State column

## Data input and output

-   **Data input**
    -   From file
    -   From google drive
    -   **From API**
    -   From database
    -   Webscrapping
-   Data output

# From API

## From API

-   Open Data
-   API (Application programming interfaces)
-   JSON格式檔案

## Open Data 開放資料

-   2011年推動開放政府與開放資料 ([維基百科](https://zh.wikipedia.org/wiki/%E9%96%8B%E6%94%BE%E8%B3%87%E6%96%99))
-   不受著作權、專利權，以及其他管理機制所限制，任何人都可以自由出版使用
-   常見的儲存方式為:
    -   `CSV`
    -   `JSON`
    -   `XML`

## Open Data 開放資料常見平台

-   [政府資料開放平台](https://data.gov.tw/)
-   [台北市資料大平台](https://data.taipei/)
-   [桃園開放資料](https://opendata.tycg.gov.tw/)
-   [新竹市政府資料開放平臺](https://opendata.hccg.gov.tw/)

## API

-   應用程式介面
-   **A**pplication **P**rogramming **I**nterfaces
-   為了讓第三方的開發者可以額外開發應用程式來強化他們的產品，推出可以與系統溝通的介面
-   有API輔助可將資料擷取過程自動化
    -   以下載Open Data為例，若檔案更新頻繁，使用手動下載相當耗時
-   [維基百科](https://zh.wikipedia.org/zh-tw/%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E6%8E%A5%E5%8F%A3)

## API - Open Data

-   [桃園公共自行車即時服務資料]https://opendata.tycg.gov.tw/datalist/5ca2bfc7-9ace-4719-88ae-4034b9a5a55c)資料
-   每日更新
-   不可能每日手動下載
-   提供透過**API**下載的服務
-   透過API下載的資料格式: **JSON格式**




## JSON format

-   JSON (**J**ava**s**cript **O**bject **N**otation)
-   輕量級的資料交換語言
-   From **a**pplication **p**rogramming **i**nterfaces (APIs)
-   JavaScript、Java、Node.js應用
-   一些NoSQL資料庫用JSON儲存資料：**MongoDB**
-   [Wiki](http://en.wikipedia.org/wiki/JSON)

![](figures/json.png){width="383"}

## JSON檔案匯入

-   從網路載資料，需要安裝`requests`套件
-   處理json資料，需要安裝與載入`json`套件

```{python}
#| echo: true
#| eval: false
!pip3 install json requests
```

```{python}
#| echo: true
import json, requests
```

## JSON檔案匯入

-   API網址參考[臺中市公共自行車(YouBike2.0)](https://opendata.taichung.gov.tw/search/6e38eb56-0e9a-4b9e-806d-23cd35d44d6b)
-   點選**API**獲取相關資訊
-   使用`requests`套件的`get(網址)`擷取資料
-   此API會檔短時間內擷取多次的要求，因此放入`params = {'param':'1'}`以及`headers = {'Connection':'close'}`設定
-   最後將使用`回傳結果.json()`，將檔案轉成JSON格式

```{python}
#| echo: true
#| cache: true
url="https://datacenter.taichung.gov.tw/swagger/OpenData/34a848ab-eeb3-44fd-a842-a09cb3209a7d"
response = requests.get(url, params = {'param':'1'}, headers = {'Connection':'close'})
data = response.json()  # Convert the response to JSON
```

## JSON檔案匯入

探索資料，使用`.keys()`查看keys

```{python}
#| echo: true
type(data)
```

```{python}
#| echo: true
data.keys()
```

```{python}
#| echo: true
type(data["retVal"])
```

## JSON檔案匯入

探索資料，發現是dist (data)中retVal key儲存含有資料的list

```{python}
#| echo: true
data["retVal"][0:2]
```

## JSON檔案匯入

探索資料，發現是dist (data)中retVal key儲存含有資料的list

```{python}
#| echo: true
data["retVal"][0]
```

## Hands-on - JSON檔案匯入練習

-   練習用資料：[YouBike2.0臺北市公共自行車即時資訊](https://data.gov.tw/dataset/137993)
-   API地址[在此](https://tcgbusfs.blob.core.windows.net/dotapp/youbike/v2/youbike_immediate.json)
-   使用檔案匯入**範例**，將資料匯入Python中
    -   提示：設定url
-   找到資料的儲存處，並將答案print出

## From API - Recap

-   Open Data
-   API (Application programming interfaces)
-   JSON格式檔案 `requests.get()`下載，`.json`轉型
-   用`type()`查看資料類別
-   若為dist，則可使用`keys()`查看資料擷取方法，並印出來試試看

## Data input and output

-   **Data input**
    -   From file
    -   From google drive
    -   From API
    -   **From database**
    -   Webscrapping
-   Data output

# From database

## From database

-   Database access
    -   File system: SQLite
    -   Database system: MySQL, MS SQL Server, ...etc
-   DuckDB
    -   Using file system
    -   select data with SQL and pandas!

## Databases access - SQLite

File system以SQLite為例，需要先安裝`sqlite3`套件

```{python}
#| echo: true
#| eval: false
!pip3 install sqlite3
```

## Databases access - SQLite

-   `dbfile` local path for db file or url
-   `sqlite3.connect(dbfile)` connect to the db file
    - `dbfile` can be local path or url
-   `conn.execute("SQL")` execute `SQL`
-   `pd.read_sql_query("SQL")` retrieve data with `SQL` and save as dataframe
-   `conn.close()` close the connection

```{python}
#| echo: true
import sqlite3
import pandas as pd
dbfile = "chinook.db"
conn = sqlite3.connect(dbfile)
df = pd.read_sql_query("select * from artists", conn)
conn.close()
```

## Databases access - SQLite

```{python}
#| echo: true
df
```

## Databases access - MySQL (FYR)

以MySQL為例，需要先安裝`mysql-connector-python`套件

```{python}
#| echo: true
#| eval: false
!pip3 install mysql-connector-python
```

## Databases access - MySQL (FYR)
```{python}
#| echo: true
#| eval: false
import mysql.connector
mydb = mysql.connector.connect(
  host="localhost",
  user="yourusername",
  password="yourpassword",
  database="mydatabase"
)
mycursor = mydb.cursor()
mycursor.execute("SELECT name, address FROM table_name")
myresult = mycursor.fetchall()
df = pd.DataFrame(myresult)
```

## DuckDB

-   https://duckdb.org/
-   an open-source column-oriented relational database management system 
-   Unlike other embedded databases (SQLite) DuckDB is not focusing on transactional (OLTP) applications and instead is specialized for online analytical processing (OLAP) workloads
-   需要先安裝`pandas`和`duckdb`套件

```{python}
#| echo: true
#| eval: false
!pip3 install pandas duckdb
```

## DuckDB
-   Load any file with DuckDB
-   Use `.csv` as an example 
    -   TSMC stock price
    -   Download from [MarketWatch](https://www.marketwatch.com/investing/stock/2330/download-data?startDate=2/1/2022&endDate=2/18/2025&countryCode=tw)
-   `file_path` can be `local file path` or `url`

```{python}
#| echo: true
import pandas as pd
import duckdb
file_path = "STOCK_TW_XTAI_2330.csv" 
```

## DuckDB

-   `duckdb.connect(database=':memory:')` open the connection
-   `conn.execute(SQL)` execute `SQL`
    -   `CREATE`, `SELECT`, etc
-   `conn.close()` close the connection
```{python}
#| echo: true
conn = duckdb.connect(database=':memory:')   
conn.execute(f"""
    CREATE TABLE TSMC_stock AS 
    SELECT *
    FROM read_csv_auto('{file_path}')
""")
```

## DuckDB

-   `conn.execute(SQL)` execute `SQL`
    -   `CREATE`, `SELECT`, etc
    -   `SELECT` retrieval can be stored in dataframe
-   `conn.close()` close the connection
-   We need more than a `*`.... for data preprocessing

```{python}
#| echo: true
df_duckdb = conn.execute("SELECT * FROM TSMC_stock").fetchdf()
conn.close()     
df_duckdb
```

## DuckDB

We need more than a `*`.... for data preprocessing

```{python}
#| echo: true
conn = duckdb.connect(database=':memory:')   
conn.execute(f"""
    CREATE TABLE TSMC_stock AS 
    SELECT 
        Date,  -- Keep the Date column unchanged
        CAST(REPLACE(Open, ',', '') AS DOUBLE) AS Open,
        CAST(REPLACE(High, ',', '') AS DOUBLE) AS High,
        CAST(REPLACE(Low, ',', '') AS DOUBLE) AS Low,
        CAST(REPLACE(Close, ',', '') AS DOUBLE) AS Close,
        CAST(REPLACE(Volume, ',', '') AS BIGINT) AS Volume
    FROM read_csv_auto('{file_path}')
""")
df_duckdb = conn.execute("SELECT * FROM TSMC_stock").fetchdf()
conn.close()     
```

## DuckDB
```{python}
#| echo: true
df_duckdb   
```

## DuckDB
You can also use `duckdb.query(SQL).to_df()` for data retrieval
```{python}
#| echo: true
Query = '''
SELECT YEAR(Date) AS Year, MONTH(Date) AS Month, AVG(Open) as avg_price, COUNT(Open) AS count 
FROM df_duckdb 
GROUP BY Year, Month
ORDER BY avg_price DESC
'''
df_duckdb2 = duckdb.query(Query).to_df()
df_duckdb2
```

## Data input and output

-   **Data input**
    -   From file
    -   From google drive
    -   From API
    -   From database
    -   **Webscrapping**
-   Data output

# 網頁爬蟲 Webscraping

## 網頁爬蟲 Webscraping

-   不是每個網站都提供API
-   人工複製貼上?!
-   程式化的方式擷取網頁資料: **網頁爬蟲（Webscraping）**（[Webscraping Wiki](http://en.wikipedia.org/wiki/Web_scraping)）
-   可能耗費很多網頁流量和資源 －很可能被鎖IP
-   使用`beautifulsoup4` 套件輔助

## 網頁爬蟲 Webscraping-beautifulsoup4

需要先安裝`beautifulsoup4`以及`requests`套件。

```{python}
#| echo: true
#| eval: false
!pip3 install beautifulsoup4 requests
```

-   擷取條件的撰寫會因網頁語法不同而有差異
-   使用**Google Chrome開發工具**輔助觀察擷取資料的條件
    -   或是按右鍵，點選“檢查”
    -   或使用**SelectorGadget**輔助
-   觀察需要擷取的資料所在HTML片段
    -   css class, id 等

## 即時股價爬取

-   [Google 財經](ttps://www.google.com/finance/)
-   [Google 財經 - 台積電](https://www.google.com/finance/quote/2330:TPE?hl=zh-TW)
```{python}
#| echo: true
from bs4 import BeautifulSoup
import requests
StockUrl = "https://www.google.com/finance/quote/2330:TPE?hl=zh-TW"
res = requests.get(StockUrl)
soup = BeautifulSoup(res.text, "html.parser")
```
```{python}
#| echo: true
price = soup.select(".fxKbKc")
price[0].get_text()
```


## 即時股價爬取 - 下載html

-   使用`requests`套件的`get(網址)`函數，輸入網址，將網頁載入python分析環境。
-   成功載入網頁後，針對該物件，使用`.text`取得網頁原始碼 (.html)

```{python}
#| echo: true
StockUrl = "https://www.google.com/finance/quote/2330:TPE?hl=zh-TW"
res = requests.get(StockUrl)
res.text[0:1000] ## print first 1000 chars
```

## 即時股價爬取 - 解析html

-   使用`BeautifulSoup`套件提供的解析方法`BeautifulSoup(html檔案,格式)`
    -   輸入剛剛載回來的html檔案`res.text`，以及設定格式`"html.parser"`，完成HTML解析
-   解析後可使用`解析後內容.prettify`取得排版後的網頁原始碼

```{python}
#| echo: true
soup = BeautifulSoup(res.text, "html.parser")
```

## 即時股價爬取 - 搜尋所需節點 -1

-   `find(節點條件)`:
    -   取得第一個符合條件的節點 (`<>`內的內容)
-   `find_all(節點條件)`
    -   取得所有符合條件的節點 (`<>`內的內容)
-   `select_one()`
    -   取得第一個符合CSS條件的節點 (`class` or `id`等屬性)
-   `select()`
    -   取得所有符合CSS條件的節點 (`class` or `id`等屬性)

## 即時股價爬取 - 搜尋所需節點 -1

範例：class 包含 `fxKbKc`

-   `select()`
    -   取得所有符合CSS條件的節點 (`class` or `id`等屬性)
-   `class=`可用`.`代表
-   `id=`可用`#`代表

```{python}
#| echo: true
soup = BeautifulSoup(res.text, "html.parser")
price = soup.select(".fxKbKc")
price
```

## 即時股價爬取 - 取出需要的資料 -1

-   `.get_text()`可抓取節點內文字
-   `.get("屬性名稱")`可抓取屬性內容

```{python}
#| echo: true
price[0].get_text()
```

## 即時股價爬取 - 搜尋所需節點 -2

範例：class 包含 `gyFHrc`

-   `select()`
    -   取得所有符合CSS條件的節點 (`class` or `id`等屬性)

```{python}
#| echo: true
soup = BeautifulSoup(res.text, "html.parser")
price_detail = soup.select(".gyFHrc")
price_detail
```

## 即時股價爬取 - 取出需要的資料 -2

-   `.get_text()`可抓取節點內文字
-   `.get("屬性名稱")`可抓取屬性內容

```{python}
#| echo: true
len(price_detail)
```

```{python}
#| echo: true
for price in price_detail:
  print(price.get_text())
```


## 即時股價爬取 - 搜尋所需節點 -3

範例：class 包含 `z4rs2b`

-   `select()`: 取得所有符合CSS條件的節點 (`class` or `id`等屬性)

```{python}
#| echo: true
soup = BeautifulSoup(res.text, "html.parser")
news = soup.select(".z4rs2b")
news
```
## 即時股價爬取 - 取出需要的資料 -3

-   `.get_text()`可抓取節點內文字
-   `.get("屬性名稱")`可抓取屬性內容

```{python}
#| echo: true
len(news)
```

```{python}
#| echo: true
for title_click in news:
  print(title_click.get_text())
```



## Hands-on 爬蟲練習

-   [Ptt Tech_Job 版](https://www.ptt.cc/bbs/Tech_Job/index.html)
-   試著爬出所有**標題**
-   爬出的第三個標題是？
-   提示：
    -   下載html `requests.get(網址)`
    -   解析html `BeautifulSoup(下載網頁.text, "html.parser")`\
    -   搜尋所需節點 `find(節點)`, `select(css條件)`
    -   取出需要的資料 `get_text()`, `get(屬性名稱)`


## 即時股價爬取 - 取出需要的資料 -4

-   `.get_text()`可抓取節點內文字
-   `.get("屬性名稱")`可抓取屬性內容

```{python}
#| echo: true
for title_click in news:
  print(title_click.find("a").get("href"))
```

## 即時股價爬取 - 爬連結內的新聞

-   得到連結後，重複上述步驟
    -   下載html `requests.get(網址)`
    -   解析html `BeautifulSoup(下載網頁.text, "html.parser")`\
    -   搜尋所需節點 `find(節點)`, `select(css條件)`
    -   取出需要的資料 `get_text()`, `get(屬性名稱)`

## 即時股價爬取 - 爬連結內的新聞

首先取得連結list。

補充內容：

-   `sequence物件.append()`可新增內容至sequence中

```{python}
#| echo: true
url_list = []
title_list = []
for title_click in news:
  url_list.append(title_click.find("a").get("href"))
  title_list.append(title_click.get_text())
print(url_list)
```

## 即時股價爬取 - 爬連結內的新聞

以第一個新聞連結為例，下載html後，重複步驟，取得本文

```{python}
#| echo: true
res = requests.get(url_list[1])
soup = BeautifulSoup(res.text, "html.parser")
news_full = soup.select("p")
full_text = ""
for news_para in news_full:
  full_text = full_text + news_para.get_text()
print("標題: "+title_list[1] +", 內文:" + full_text)
```


## 即時股價爬取 - 爬連結內的新聞

用`for`迴圈一次做完所有連結，並轉成pandas data frame

```{python}
#| echo: true
newdf = []
for title_click in news:
  title = title_click.get_text()
  url = title_click.find("a").get("href")
  res = requests.get(url)
  soup = BeautifulSoup(res.text, "html.parser")
  news_full = soup.select("p")
  full_text = ""
  for news_para in news_full:
    full_text = full_text + news_para.get_text()
  add_news = pd.DataFrame({"title":[title],"content":[full_text]})
  newdf.append(add_news)

newdf_pd=pd.concat(newdf)
newdf_pd
```

## 即時股價爬取 - 爬連結內的新聞

```{python}
newdf_pd
```

## Hands-on 爬蟲練習

-   [Ptt Tech_Job 版](https://www.ptt.cc/bbs/Tech_Job/index.html)
-   試著爬出**最新的一頁**、**前一頁**以及**前前一頁**的所有**標題**和**內文網址**
-   總共爬出幾個標題？
-   提示：
    -   下載html `requests.get(網址)`
    -   解析html `BeautifulSoup(下載網頁.text, "html.parser")`\
    -   搜尋所需節點 `find(節點)`, `select(css條件)`
    -   取出需要的資料 `get_text()`, `get(屬性名稱)`
-   一頁一頁爬 or for 迴圈都可以

## 網頁爬蟲 進階版

-   滾動式等動態網頁網頁，如剛剛的新聞
    -   [Python 進階爬蟲 by Andy Chiang](https://hackmd.io/@AndyChiang/DynamicCrawler)

## 網頁爬蟲 再想想？

-   其實... 很多資料有其他存取方式，像API
-   隱私問題 （OkCupid事件）
    -   [70,000 OkCupid Users Just Had Their Data Published](https://motherboard.vice.com/en_us/article/70000-okcupid-users-just-had-their-data-published)

## 網頁爬蟲 Webscraping - Recap

-   下載html `requests.get(網址)`
-   解析html `BeautifulSoup(下載網頁.text, "html.parser")`
-   搜尋所需節點 `find(節點)`, `select(css條件)`
-   取出需要的資料 `get_text()`, `get(屬性名稱)`

## Data input and output

-   Data input
    -   From file
    -   From google drive
    -   From API
    -   From database
    -   Webscrapping
-   **Data output**

# 資料匯出

## 資料匯出

-   文字檔 .txt
-   CSV檔 .csv
-   json檔 .json

## 資料匯出 - 文字檔 .txt

-   首先使用`open(檔案名稱)`開啟檔案
-   使用`檔案.writelines(資料)`寫入內容
-   `檔案.writelines(資料)`的資料可以是列表

```{python}
#| echo: true
path = 'output_writeline.txt'
f = open(path, 'w')
lines1 = ['Hello World', '123', '456', '789']
lines2 = ['Hello World\n', '123\n', '456\n', '789\n']
f.writelines(lines1)
f.writelines(lines2)
f.close()
```

## 資料匯出 - 文字檔 .txt

-   首先使用`open(檔案名稱)`開啟檔案
-   使用`檔案.write(資料)`寫入內容
-   `檔案.write(資料)`的資料必須是文字

```{python}
#| echo: true
path = 'output_write.txt'
f = open(path, 'w')
for t in lines1:
  f.write(t)
  
for t in lines2:
  f.write(t)

f.close()
```

## 資料匯出 - CSV檔 .csv

使用`pandas` Data frame物件中的`.to_csv(檔名)`儲存檔案。

```{python}
#| echo: true
house.to_csv("house.csv")
```

## 資料匯出 - json檔 .json

使用`pandas` Data frame物件中的`.to_json(檔名)`儲存檔案。

DataFrame中一定要有index

```{python}
#| echo: true
house.to_json("house.json")
house
```

## Hands-on 資料匯出

-   [Ptt Tech_Job 版](https://www.ptt.cc/bbs/Tech_Job/index.html)
-   試著爬出最新一頁的所有**標題** (Hint!)
-   將標題存成一個資料框
-   儲存標題資料框，檔名為ptt.csv

## Hands-on 資料匯出

```{python}
#| echo: true
from bs4 import BeautifulSoup
import requests
import pandas as pd
# step 1 download url
res = requests.get("https://www.ptt.cc/bbs/Tech_Job/index.html")
# step 2 parse html
soup = BeautifulSoup(res.text, "html.parser")
# step 3 select tags/css
title_tags = soup.select(".title a")
print(title_tags)
# step 4 get the data
title_list=[] 
for title in title_tags:
  title_list.append(title.get_text())
```

## 資料匯出 - Recap

-   文字檔 .txt `writelines(內容)`
-   CSV檔 .csv `pandas_df.to_csv(檔名)`
-   json檔 .json `pandas_df.to_json(檔名)`

## References

-   [Python for Everybody](https://www.py4e.com/)
    -   Some contents are from Python for Everybody, and these contents are Copyright 2010- Charles R. Severance (www.dr-chuck.com) of the University of Michigan School of Information and open.umich.edu and made available under a Creative Commons Attribution 4.0 License.
-   [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)

# Questions?

